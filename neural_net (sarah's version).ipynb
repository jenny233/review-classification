{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20000, 1000)\n",
      "y_train shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "# Load training text file\n",
    "Data_train = np.loadtxt('training_data.txt', skiprows = 1)\n",
    "X_train = Data_train[:, 1:]\n",
    "y_train = Data_train[:, 0]\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "trainx = X_train[0:10000]\n",
    "trainy = y_train[0:10000]\n",
    "testx = X_train[10000:]\n",
    "testy = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train[0])):\n",
    "    col = X_train[:, i]\n",
    "    mean = np.mean(col)\n",
    "    std = np.std(col)\n",
    "    X_train[:, i] = (col - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_121 (Dense)            (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 107,751\n",
      "Trainable params: 107,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from keras import regularizers\n",
    "\n",
    "# Build sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(100, activation=\"sigmoid\", input_shape=(1000,)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(50, activation=\"sigmoid\"))\n",
    "model.add(Dropout(.4))\n",
    "model.add(Dense(50, activation = 'sigmoid'))\n",
    "model.add(Dropout(.4))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "# Print a summary\n",
    "model.summary()\n",
    "# Compile\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/12\n",
      "16000/16000 [==============================] - 4s 222us/step - loss: 0.6953 - acc: 0.5652 - val_loss: 0.5135 - val_acc: 0.8190\n",
      "Epoch 2/12\n",
      "16000/16000 [==============================] - 1s 67us/step - loss: 0.4259 - acc: 0.8185 - val_loss: 0.3515 - val_acc: 0.8498\n",
      "Epoch 3/12\n",
      "16000/16000 [==============================] - 1s 66us/step - loss: 0.3553 - acc: 0.8569 - val_loss: 0.3498 - val_acc: 0.8560\n",
      "Epoch 4/12\n",
      "16000/16000 [==============================] - 1s 72us/step - loss: 0.3387 - acc: 0.8660 - val_loss: 0.3510 - val_acc: 0.8525\n",
      "Epoch 5/12\n",
      "16000/16000 [==============================] - 1s 77us/step - loss: 0.3185 - acc: 0.8746 - val_loss: 0.3518 - val_acc: 0.8535\n",
      "Epoch 6/12\n",
      "16000/16000 [==============================] - 1s 85us/step - loss: 0.3086 - acc: 0.8763 - val_loss: 0.3551 - val_acc: 0.8562\n",
      "Epoch 7/12\n",
      "16000/16000 [==============================] - 1s 88us/step - loss: 0.2998 - acc: 0.8779 - val_loss: 0.3561 - val_acc: 0.8542\n",
      "Epoch 8/12\n",
      "16000/16000 [==============================] - 1s 90us/step - loss: 0.2948 - acc: 0.8815 - val_loss: 0.3566 - val_acc: 0.8520\n",
      "Epoch 9/12\n",
      "16000/16000 [==============================] - 1s 77us/step - loss: 0.2842 - acc: 0.8820 - val_loss: 0.3584 - val_acc: 0.8528\n",
      "Epoch 10/12\n",
      "16000/16000 [==============================] - 1s 73us/step - loss: 0.2797 - acc: 0.8888 - val_loss: 0.3617 - val_acc: 0.8540\n",
      "Epoch 11/12\n",
      "16000/16000 [==============================] - 1s 82us/step - loss: 0.2715 - acc: 0.8871 - val_loss: 0.3622 - val_acc: 0.8530\n",
      "Epoch 12/12\n",
      "16000/16000 [==============================] - 1s 77us/step - loss: 0.2618 - acc: 0.8919 - val_loss: 0.3688 - val_acc: 0.8515\n"
     ]
    }
   ],
   "source": [
    "fit = model.fit(X_train, y_train, batch_size=128, epochs=12, validation_split=0.2, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 97us/step\n",
      "Test score: 0.24788489606380462\n",
      "Test accuracy: 0.9018\n"
     ]
    }
   ],
   "source": [
    "## Printing the accuracy of our model, according to the loss function specified in model.compile above\n",
    "score = model.evaluate(X_train, y_train, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Comparing different dropout values on one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 550,701\n",
      "Trainable params: 550,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 5s 282us/step - loss: 0.4008 - acc: 0.8190 - val_loss: 0.3577 - val_acc: 0.8405\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 4s 275us/step - loss: 0.3306 - acc: 0.8581 - val_loss: 0.3567 - val_acc: 0.8470\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 4s 257us/step - loss: 0.3055 - acc: 0.8673 - val_loss: 0.3502 - val_acc: 0.8508\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 4s 239us/step - loss: 0.2837 - acc: 0.8752 - val_loss: 0.3570 - val_acc: 0.8505\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 4s 242us/step - loss: 0.2602 - acc: 0.8885 - val_loss: 0.3593 - val_acc: 0.8488\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 4s 244us/step - loss: 0.2290 - acc: 0.9067 - val_loss: 0.3644 - val_acc: 0.8455\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 4s 244us/step - loss: 0.1964 - acc: 0.9265 - val_loss: 0.3718 - val_acc: 0.8492\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 4s 253us/step - loss: 0.1577 - acc: 0.9484 - val_loss: 0.3898 - val_acc: 0.8445\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 4s 232us/step - loss: 0.1220 - acc: 0.9639 - val_loss: 0.4160 - val_acc: 0.8420\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 4s 225us/step - loss: 0.0928 - acc: 0.9769 - val_loss: 0.4336 - val_acc: 0.8363\n",
      "20000/20000 [==============================] - 2s 122us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 550,701\n",
      "Trainable params: 550,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 5s 298us/step - loss: 0.4026 - acc: 0.8165 - val_loss: 0.3583 - val_acc: 0.8452\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 4s 247us/step - loss: 0.3276 - acc: 0.8591 - val_loss: 0.3490 - val_acc: 0.8470\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 4s 252us/step - loss: 0.3070 - acc: 0.8661 - val_loss: 0.3512 - val_acc: 0.8502\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 4s 252us/step - loss: 0.2887 - acc: 0.8717 - val_loss: 0.3637 - val_acc: 0.8480\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 4s 276us/step - loss: 0.2693 - acc: 0.8841 - val_loss: 0.3538 - val_acc: 0.8480\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 4s 277us/step - loss: 0.2472 - acc: 0.8944 - val_loss: 0.3730 - val_acc: 0.8458\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 4s 253us/step - loss: 0.2209 - acc: 0.9106 - val_loss: 0.3777 - val_acc: 0.8438\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 4s 251us/step - loss: 0.1917 - acc: 0.9274 - val_loss: 0.3903 - val_acc: 0.8375\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 4s 250us/step - loss: 0.1604 - acc: 0.9437 - val_loss: 0.4100 - val_acc: 0.8395\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 4s 249us/step - loss: 0.1322 - acc: 0.9577 - val_loss: 0.4179 - val_acc: 0.8347\n",
      "20000/20000 [==============================] - 3s 126us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 550,701\n",
      "Trainable params: 550,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 5s 303us/step - loss: 0.4041 - acc: 0.8177 - val_loss: 0.3575 - val_acc: 0.8420\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 4s 248us/step - loss: 0.3302 - acc: 0.8575 - val_loss: 0.3551 - val_acc: 0.8538\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 4s 253us/step - loss: 0.3086 - acc: 0.8646 - val_loss: 0.3506 - val_acc: 0.8535\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 4s 254us/step - loss: 0.2911 - acc: 0.8716 - val_loss: 0.3518 - val_acc: 0.8530\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 4s 250us/step - loss: 0.2759 - acc: 0.8780 - val_loss: 0.3622 - val_acc: 0.8462\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 4s 254us/step - loss: 0.2531 - acc: 0.8924 - val_loss: 0.3608 - val_acc: 0.8515\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 4s 256us/step - loss: 0.2299 - acc: 0.9024 - val_loss: 0.3718 - val_acc: 0.8455\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 4s 246us/step - loss: 0.2029 - acc: 0.9197 - val_loss: 0.3779 - val_acc: 0.8455\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 4s 244us/step - loss: 0.1751 - acc: 0.9360 - val_loss: 0.3947 - val_acc: 0.8425\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 4s 242us/step - loss: 0.1465 - acc: 0.9501 - val_loss: 0.4080 - val_acc: 0.8390\n",
      "20000/20000 [==============================] - 2s 119us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 550,701\n",
      "Trainable params: 550,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 5s 292us/step - loss: 0.4054 - acc: 0.8175 - val_loss: 0.3544 - val_acc: 0.8490\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 4s 249us/step - loss: 0.3330 - acc: 0.8556 - val_loss: 0.3486 - val_acc: 0.8512\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 4s 248us/step - loss: 0.3141 - acc: 0.8618 - val_loss: 0.3545 - val_acc: 0.8465\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 4s 249us/step - loss: 0.2949 - acc: 0.8708 - val_loss: 0.3509 - val_acc: 0.8492\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 4s 247us/step - loss: 0.2793 - acc: 0.8782 - val_loss: 0.3536 - val_acc: 0.8498\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 4s 246us/step - loss: 0.2622 - acc: 0.8874 - val_loss: 0.3625 - val_acc: 0.8490\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 4s 251us/step - loss: 0.2408 - acc: 0.8964 - val_loss: 0.3696 - val_acc: 0.8435\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 4s 245us/step - loss: 0.2191 - acc: 0.9119 - val_loss: 0.3800 - val_acc: 0.8430\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 4s 248us/step - loss: 0.1949 - acc: 0.9231 - val_loss: 0.3950 - val_acc: 0.8397\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 4s 249us/step - loss: 0.1688 - acc: 0.9374 - val_loss: 0.4110 - val_acc: 0.8387\n",
      "20000/20000 [==============================] - 2s 124us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 550,701\n",
      "Trainable params: 550,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 5s 316us/step - loss: 0.4075 - acc: 0.8160 - val_loss: 0.3637 - val_acc: 0.8410\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 4s 249us/step - loss: 0.3372 - acc: 0.8527 - val_loss: 0.3549 - val_acc: 0.8490\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 4s 250us/step - loss: 0.3172 - acc: 0.8609 - val_loss: 0.3506 - val_acc: 0.8542\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 4s 249us/step - loss: 0.3031 - acc: 0.8678 - val_loss: 0.3500 - val_acc: 0.8530\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 4s 251us/step - loss: 0.2898 - acc: 0.8743 - val_loss: 0.3530 - val_acc: 0.8515\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 4s 253us/step - loss: 0.2781 - acc: 0.8791 - val_loss: 0.3555 - val_acc: 0.8508\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 4s 269us/step - loss: 0.2618 - acc: 0.8862 - val_loss: 0.3589 - val_acc: 0.8520\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 5s 298us/step - loss: 0.2463 - acc: 0.8950 - val_loss: 0.3687 - val_acc: 0.8468\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 5s 284us/step - loss: 0.2274 - acc: 0.9032 - val_loss: 0.3714 - val_acc: 0.8452\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 5s 308us/step - loss: 0.2092 - acc: 0.9132 - val_loss: 0.3969 - val_acc: 0.8417\n",
      "20000/20000 [==============================] - 3s 141us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 550,701\n",
      "Trainable params: 550,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 6s 355us/step - loss: 0.4124 - acc: 0.8089 - val_loss: 0.3458 - val_acc: 0.8540\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 5s 291us/step - loss: 0.3397 - acc: 0.8533 - val_loss: 0.3559 - val_acc: 0.8470\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 4s 273us/step - loss: 0.3241 - acc: 0.8589 - val_loss: 0.3506 - val_acc: 0.8508\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 4s 262us/step - loss: 0.3129 - acc: 0.8626 - val_loss: 0.3485 - val_acc: 0.8525\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 4s 267us/step - loss: 0.2975 - acc: 0.8690 - val_loss: 0.3503 - val_acc: 0.8478\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 4s 265us/step - loss: 0.2881 - acc: 0.8746 - val_loss: 0.3564 - val_acc: 0.8535\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 4s 277us/step - loss: 0.2805 - acc: 0.8772 - val_loss: 0.3598 - val_acc: 0.8545\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 4s 266us/step - loss: 0.2679 - acc: 0.8806 - val_loss: 0.3679 - val_acc: 0.8510\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 4s 274us/step - loss: 0.2551 - acc: 0.8899 - val_loss: 0.3789 - val_acc: 0.8498\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 4s 257us/step - loss: 0.2461 - acc: 0.8939 - val_loss: 0.3765 - val_acc: 0.8482\n",
      "20000/20000 [==============================] - 3s 133us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 550,701\n",
      "Trainable params: 550,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 5s 343us/step - loss: 0.4243 - acc: 0.8029 - val_loss: 0.3533 - val_acc: 0.8490\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 5s 308us/step - loss: 0.3472 - acc: 0.8496 - val_loss: 0.3492 - val_acc: 0.8465\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 4s 273us/step - loss: 0.3267 - acc: 0.8598 - val_loss: 0.3467 - val_acc: 0.8502\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 5s 285us/step - loss: 0.3178 - acc: 0.8617 - val_loss: 0.3452 - val_acc: 0.8542\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 4s 276us/step - loss: 0.3066 - acc: 0.8683 - val_loss: 0.3460 - val_acc: 0.8532\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 4s 275us/step - loss: 0.2972 - acc: 0.8699 - val_loss: 0.3507 - val_acc: 0.8520\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 4s 259us/step - loss: 0.2898 - acc: 0.8706 - val_loss: 0.3514 - val_acc: 0.8498\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 4s 268us/step - loss: 0.2788 - acc: 0.8772 - val_loss: 0.3658 - val_acc: 0.8542\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 4s 249us/step - loss: 0.2734 - acc: 0.8795 - val_loss: 0.3606 - val_acc: 0.8482\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 4s 246us/step - loss: 0.2645 - acc: 0.8850 - val_loss: 0.3703 - val_acc: 0.8452\n",
      "20000/20000 [==============================] - 3s 128us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 550,701\n",
      "Trainable params: 550,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 5s 313us/step - loss: 0.4398 - acc: 0.7938 - val_loss: 0.3563 - val_acc: 0.8482\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 4s 271us/step - loss: 0.3533 - acc: 0.8436 - val_loss: 0.3537 - val_acc: 0.8542\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 4s 277us/step - loss: 0.3389 - acc: 0.8510 - val_loss: 0.3456 - val_acc: 0.8542\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 4s 248us/step - loss: 0.3206 - acc: 0.8606 - val_loss: 0.3495 - val_acc: 0.8520\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 4s 254us/step - loss: 0.3149 - acc: 0.8626 - val_loss: 0.3480 - val_acc: 0.8552\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 4s 257us/step - loss: 0.3106 - acc: 0.8644 - val_loss: 0.3466 - val_acc: 0.8508\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 5s 299us/step - loss: 0.2997 - acc: 0.8678 - val_loss: 0.3501 - val_acc: 0.8578\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 4s 278us/step - loss: 0.2963 - acc: 0.8714 - val_loss: 0.3523 - val_acc: 0.8520\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 4s 263us/step - loss: 0.2886 - acc: 0.8726 - val_loss: 0.3570 - val_acc: 0.8538\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 4s 255us/step - loss: 0.2857 - acc: 0.8739 - val_loss: 0.3563 - val_acc: 0.8540\n",
      "20000/20000 [==============================] - 3s 130us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 550,701\n",
      "Trainable params: 550,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 5s 339us/step - loss: 0.4606 - acc: 0.7776 - val_loss: 0.3542 - val_acc: 0.8492\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 4s 264us/step - loss: 0.3692 - acc: 0.8379 - val_loss: 0.3511 - val_acc: 0.8490\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 4s 271us/step - loss: 0.3468 - acc: 0.8464 - val_loss: 0.3472 - val_acc: 0.8518\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 4s 277us/step - loss: 0.3321 - acc: 0.8548 - val_loss: 0.3476 - val_acc: 0.8552\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 4s 268us/step - loss: 0.3276 - acc: 0.8550 - val_loss: 0.3480 - val_acc: 0.8535\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 5s 296us/step - loss: 0.3183 - acc: 0.8618 - val_loss: 0.3561 - val_acc: 0.8548\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 4s 267us/step - loss: 0.3146 - acc: 0.8628 - val_loss: 0.3502 - val_acc: 0.8545\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 4s 253us/step - loss: 0.3084 - acc: 0.8639 - val_loss: 0.3511 - val_acc: 0.8538\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 4s 254us/step - loss: 0.3070 - acc: 0.8686 - val_loss: 0.3497 - val_acc: 0.8540\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 4s 253us/step - loss: 0.3037 - acc: 0.8666 - val_loss: 0.3467 - val_acc: 0.8538\n",
      "20000/20000 [==============================] - 3s 131us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 550,701\n",
      "Trainable params: 550,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 5s 337us/step - loss: 0.5187 - acc: 0.7361 - val_loss: 0.3685 - val_acc: 0.8420\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 4s 258us/step - loss: 0.4017 - acc: 0.8196 - val_loss: 0.3568 - val_acc: 0.8545\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 4s 254us/step - loss: 0.3741 - acc: 0.8326 - val_loss: 0.3542 - val_acc: 0.8578\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 4s 258us/step - loss: 0.3578 - acc: 0.8444 - val_loss: 0.3533 - val_acc: 0.8562\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 4s 256us/step - loss: 0.3484 - acc: 0.8491 - val_loss: 0.3564 - val_acc: 0.8552\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 4s 254us/step - loss: 0.3412 - acc: 0.8512 - val_loss: 0.3607 - val_acc: 0.8560\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 4s 256us/step - loss: 0.3313 - acc: 0.8576 - val_loss: 0.3631 - val_acc: 0.8552\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 4s 276us/step - loss: 0.3293 - acc: 0.8575 - val_loss: 0.3650 - val_acc: 0.8575\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 4s 265us/step - loss: 0.3231 - acc: 0.8616 - val_loss: 0.3567 - val_acc: 0.8558\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 4s 263us/step - loss: 0.3225 - acc: 0.8612 - val_loss: 0.3659 - val_acc: 0.8585\n",
      "20000/20000 [==============================] - 3s 161us/step\n"
     ]
    }
   ],
   "source": [
    "scores = [0]*10\n",
    "accuracy = [0]*10\n",
    "for i in range(10):\n",
    "    # Build sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Hidden layers\n",
    "    model.add(Dense(500, activation=\"tanh\", input_shape=(1000,)))\n",
    "    model.add(Dropout(0.1 * i))\n",
    "    model.add(Dense(100, activation=\"tanh\"))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # Print a summary\n",
    "    model.summary()\n",
    "    # Compile\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    fit = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2, shuffle=True, verbose=1)\n",
    "    score = model.evaluate(X_train, y_train, verbose=1)\n",
    "    scores[i] = score[0]\n",
    "    accuracy[i] = score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f48fd26198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arange = np.multiply(range(10), .1)\n",
    "plt.plot(arange,scores, label = 'test score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhb6ISAj5/6FGaXyoGIkLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f48be594e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(arange,accuracy, label = 'test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4092 - acc: 0.8168 - val_loss: 0.3541 - val_acc: 0.8445\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 438us/step - loss: 0.3090 - acc: 0.8692 - val_loss: 0.3523 - val_acc: 0.8508\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 432us/step - loss: 0.2603 - acc: 0.8931 - val_loss: 0.3655 - val_acc: 0.8498\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 406us/step - loss: 0.2079 - acc: 0.9186 - val_loss: 0.3884 - val_acc: 0.8427\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 371us/step - loss: 0.1486 - acc: 0.9504 - val_loss: 0.4162 - val_acc: 0.8353\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 5s 307us/step - loss: 0.0949 - acc: 0.9736 - val_loss: 0.4722 - val_acc: 0.8353\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 5s 313us/step - loss: 0.0542 - acc: 0.9892 - val_loss: 0.5098 - val_acc: 0.8305\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 5s 319us/step - loss: 0.0306 - acc: 0.9958 - val_loss: 0.5696 - val_acc: 0.8313\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 5s 302us/step - loss: 0.0171 - acc: 0.9984 - val_loss: 0.6364 - val_acc: 0.8333\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 367us/step - loss: 0.0098 - acc: 0.9994 - val_loss: 0.6777 - val_acc: 0.8333\n",
      "20000/20000 [==============================] - 7s 333us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 16s 999us/step - loss: 0.4057 - acc: 0.8207 - val_loss: 0.3543 - val_acc: 0.8498\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 348us/step - loss: 0.3086 - acc: 0.8671 - val_loss: 0.3527 - val_acc: 0.8490\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 5s 326us/step - loss: 0.2643 - acc: 0.8894 - val_loss: 0.3575 - val_acc: 0.8498\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 5s 323us/step - loss: 0.2078 - acc: 0.9188 - val_loss: 0.3739 - val_acc: 0.8455\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 5s 320us/step - loss: 0.1543 - acc: 0.9463 - val_loss: 0.4067 - val_acc: 0.8377\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 5s 322us/step - loss: 0.1015 - acc: 0.9705 - val_loss: 0.4557 - val_acc: 0.8393\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 352us/step - loss: 0.0623 - acc: 0.9857 - val_loss: 0.5329 - val_acc: 0.8313\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 5s 322us/step - loss: 0.0379 - acc: 0.9937 - val_loss: 0.5822 - val_acc: 0.8315\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 5s 321us/step - loss: 0.0224 - acc: 0.9972 - val_loss: 0.6304 - val_acc: 0.8327\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 5s 318us/step - loss: 0.0149 - acc: 0.9989 - val_loss: 0.6746 - val_acc: 0.8380\n",
      "20000/20000 [==============================] - 6s 290us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 14s 888us/step - loss: 0.4079 - acc: 0.8162 - val_loss: 0.3513 - val_acc: 0.8498\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 5s 325us/step - loss: 0.3086 - acc: 0.8669 - val_loss: 0.3552 - val_acc: 0.8500\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 5s 324us/step - loss: 0.2626 - acc: 0.8898 - val_loss: 0.3634 - val_acc: 0.8472\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 5s 327us/step - loss: 0.2120 - acc: 0.9180 - val_loss: 0.3796 - val_acc: 0.8450\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 354us/step - loss: 0.1527 - acc: 0.9480 - val_loss: 0.4214 - val_acc: 0.8320\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 369us/step - loss: 0.1048 - acc: 0.9684 - val_loss: 0.4758 - val_acc: 0.8303\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 361us/step - loss: 0.0682 - acc: 0.9831 - val_loss: 0.5313 - val_acc: 0.8260\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 5s 344us/step - loss: 0.0426 - acc: 0.9906 - val_loss: 0.5895 - val_acc: 0.8290\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 5s 325us/step - loss: 0.0269 - acc: 0.9967 - val_loss: 0.6300 - val_acc: 0.8297\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 5s 331us/step - loss: 0.0189 - acc: 0.9979 - val_loss: 0.6884 - val_acc: 0.8255\n",
      "20000/20000 [==============================] - 8s 378us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 15s 911us/step - loss: 0.4074 - acc: 0.8144 - val_loss: 0.3562 - val_acc: 0.8462\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 5s 335us/step - loss: 0.3060 - acc: 0.8698 - val_loss: 0.3508 - val_acc: 0.8540\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 5s 338us/step - loss: 0.2589 - acc: 0.8906 - val_loss: 0.3662 - val_acc: 0.8450\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 5s 337us/step - loss: 0.2082 - acc: 0.9223 - val_loss: 0.3716 - val_acc: 0.8460\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 5s 339us/step - loss: 0.1534 - acc: 0.9459 - val_loss: 0.4180 - val_acc: 0.8468\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 405us/step - loss: 0.1058 - acc: 0.9674 - val_loss: 0.4661 - val_acc: 0.8425\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 362us/step - loss: 0.0693 - acc: 0.9817 - val_loss: 0.5108 - val_acc: 0.8367\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 347us/step - loss: 0.0458 - acc: 0.9912 - val_loss: 0.5980 - val_acc: 0.8397\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 352us/step - loss: 0.0299 - acc: 0.9951 - val_loss: 0.6416 - val_acc: 0.8380\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 352us/step - loss: 0.0217 - acc: 0.9971 - val_loss: 0.6746 - val_acc: 0.8360\n",
      "20000/20000 [==============================] - 7s 327us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 14s 890us/step - loss: 0.4075 - acc: 0.8152 - val_loss: 0.3560 - val_acc: 0.8492\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 352us/step - loss: 0.3093 - acc: 0.8681 - val_loss: 0.3520 - val_acc: 0.8495\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 354us/step - loss: 0.2675 - acc: 0.8875 - val_loss: 0.3579 - val_acc: 0.8495\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 356us/step - loss: 0.2168 - acc: 0.9158 - val_loss: 0.3720 - val_acc: 0.8460\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 349us/step - loss: 0.1635 - acc: 0.9426 - val_loss: 0.4090 - val_acc: 0.8375\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 353us/step - loss: 0.1144 - acc: 0.9638 - val_loss: 0.4471 - val_acc: 0.8385\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 368us/step - loss: 0.0778 - acc: 0.9789 - val_loss: 0.5099 - val_acc: 0.8355\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 405us/step - loss: 0.0540 - acc: 0.9874 - val_loss: 0.5618 - val_acc: 0.8277\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 440us/step - loss: 0.0394 - acc: 0.9914 - val_loss: 0.5962 - val_acc: 0.8257\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 358us/step - loss: 0.0288 - acc: 0.9937 - val_loss: 0.6647 - val_acc: 0.8285\n",
      "20000/20000 [==============================] - 6s 303us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 15s 948us/step - loss: 0.4081 - acc: 0.8144 - val_loss: 0.3567 - val_acc: 0.8468\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 351us/step - loss: 0.3077 - acc: 0.8688 - val_loss: 0.3535 - val_acc: 0.8445\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 344us/step - loss: 0.2634 - acc: 0.8906 - val_loss: 0.3592 - val_acc: 0.8448\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 345us/step - loss: 0.2132 - acc: 0.9152 - val_loss: 0.3665 - val_acc: 0.8518\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 5s 321us/step - loss: 0.1633 - acc: 0.9432 - val_loss: 0.4008 - val_acc: 0.8478\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 5s 335us/step - loss: 0.1157 - acc: 0.9630 - val_loss: 0.4409 - val_acc: 0.8423\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 5s 331us/step - loss: 0.0780 - acc: 0.9781 - val_loss: 0.4954 - val_acc: 0.8325\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 5s 331us/step - loss: 0.0572 - acc: 0.9854 - val_loss: 0.5522 - val_acc: 0.8347\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 5s 319us/step - loss: 0.0404 - acc: 0.9904 - val_loss: 0.5782 - val_acc: 0.8317\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 5s 337us/step - loss: 0.0305 - acc: 0.9933 - val_loss: 0.6503 - val_acc: 0.8327\n",
      "20000/20000 [==============================] - 6s 291us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 14s 886us/step - loss: 0.4095 - acc: 0.8137 - val_loss: 0.3490 - val_acc: 0.8532\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 5s 331us/step - loss: 0.3090 - acc: 0.8679 - val_loss: 0.3493 - val_acc: 0.8518\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 5s 328us/step - loss: 0.2658 - acc: 0.8899 - val_loss: 0.3533 - val_acc: 0.8495\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 5s 334us/step - loss: 0.2223 - acc: 0.9129 - val_loss: 0.3661 - val_acc: 0.8460\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 5s 333us/step - loss: 0.1732 - acc: 0.9361 - val_loss: 0.3937 - val_acc: 0.8433\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 5s 340us/step - loss: 0.1314 - acc: 0.9554 - val_loss: 0.4278 - val_acc: 0.8407\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 5s 332us/step - loss: 0.0955 - acc: 0.9711 - val_loss: 0.4571 - val_acc: 0.8390\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 5s 330us/step - loss: 0.0647 - acc: 0.9838 - val_loss: 0.5254 - val_acc: 0.8413\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 5s 328us/step - loss: 0.0471 - acc: 0.9888 - val_loss: 0.5774 - val_acc: 0.8375\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 347us/step - loss: 0.0346 - acc: 0.9926 - val_loss: 0.6170 - val_acc: 0.8365\n",
      "20000/20000 [==============================] - 6s 295us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 14s 897us/step - loss: 0.4110 - acc: 0.8141 - val_loss: 0.3560 - val_acc: 0.8462\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 5s 331us/step - loss: 0.3106 - acc: 0.8691 - val_loss: 0.3519 - val_acc: 0.8512\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 5s 334us/step - loss: 0.2708 - acc: 0.8876 - val_loss: 0.3636 - val_acc: 0.8470\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 5s 331us/step - loss: 0.2292 - acc: 0.9092 - val_loss: 0.3642 - val_acc: 0.8415\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 5s 332us/step - loss: 0.1832 - acc: 0.9321 - val_loss: 0.3888 - val_acc: 0.8455\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 5s 330us/step - loss: 0.1354 - acc: 0.9532 - val_loss: 0.4394 - val_acc: 0.8410\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 5s 335us/step - loss: 0.1002 - acc: 0.9682 - val_loss: 0.4611 - val_acc: 0.8407\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 5s 332us/step - loss: 0.0712 - acc: 0.9801 - val_loss: 0.5234 - val_acc: 0.8390\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 5s 331us/step - loss: 0.0541 - acc: 0.9858 - val_loss: 0.5849 - val_acc: 0.8345\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 344us/step - loss: 0.0416 - acc: 0.9898 - val_loss: 0.6258 - val_acc: 0.8293\n",
      "20000/20000 [==============================] - 6s 294us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 15s 952us/step - loss: 0.4104 - acc: 0.8138 - val_loss: 0.3539 - val_acc: 0.8475\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 5s 342us/step - loss: 0.3121 - acc: 0.8663 - val_loss: 0.3545 - val_acc: 0.8472\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 5s 344us/step - loss: 0.2742 - acc: 0.8852 - val_loss: 0.3735 - val_acc: 0.8468\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 346us/step - loss: 0.2339 - acc: 0.9059 - val_loss: 0.3723 - val_acc: 0.8492\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 5s 342us/step - loss: 0.1892 - acc: 0.9273 - val_loss: 0.3901 - val_acc: 0.8492\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 346us/step - loss: 0.1427 - acc: 0.9499 - val_loss: 0.4256 - val_acc: 0.8347\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 5s 341us/step - loss: 0.1061 - acc: 0.9653 - val_loss: 0.4872 - val_acc: 0.8390\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 5s 342us/step - loss: 0.0784 - acc: 0.9761 - val_loss: 0.4934 - val_acc: 0.8307\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 5s 342us/step - loss: 0.0587 - acc: 0.9834 - val_loss: 0.5618 - val_acc: 0.8377\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 359us/step - loss: 0.0471 - acc: 0.9876 - val_loss: 0.6055 - val_acc: 0.8335\n",
      "20000/20000 [==============================] - 6s 294us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 15s 927us/step - loss: 0.4094 - acc: 0.8159 - val_loss: 0.3483 - val_acc: 0.8510\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 5s 342us/step - loss: 0.3153 - acc: 0.8662 - val_loss: 0.3546 - val_acc: 0.8480\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 5s 341us/step - loss: 0.2738 - acc: 0.8861 - val_loss: 0.3545 - val_acc: 0.8528\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 346us/step - loss: 0.2361 - acc: 0.9056 - val_loss: 0.3761 - val_acc: 0.8465\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 352us/step - loss: 0.1957 - acc: 0.9253 - val_loss: 0.3955 - val_acc: 0.8435\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 346us/step - loss: 0.1498 - acc: 0.9464 - val_loss: 0.4276 - val_acc: 0.8445\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 5s 333us/step - loss: 0.1160 - acc: 0.9621 - val_loss: 0.4627 - val_acc: 0.8375\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 5s 332us/step - loss: 0.0900 - acc: 0.9705 - val_loss: 0.5083 - val_acc: 0.8287\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 5s 334us/step - loss: 0.0642 - acc: 0.9814 - val_loss: 0.5643 - val_acc: 0.8335\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 5s 336us/step - loss: 0.0544 - acc: 0.9844 - val_loss: 0.6123 - val_acc: 0.8313\n",
      "20000/20000 [==============================] - 6s 287us/step\n",
      "0 9\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 15s 930us/step - loss: 0.4074 - acc: 0.8176 - val_loss: 0.3556 - val_acc: 0.8485\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 355us/step - loss: 0.3094 - acc: 0.8684 - val_loss: 0.3446 - val_acc: 0.8528\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 358us/step - loss: 0.2634 - acc: 0.8915 - val_loss: 0.3533 - val_acc: 0.8502\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 361us/step - loss: 0.2070 - acc: 0.9208 - val_loss: 0.3663 - val_acc: 0.8478\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 367us/step - loss: 0.1503 - acc: 0.9513 - val_loss: 0.4109 - val_acc: 0.8365\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 356us/step - loss: 0.0984 - acc: 0.9714 - val_loss: 0.4736 - val_acc: 0.8387\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 355us/step - loss: 0.0594 - acc: 0.9866 - val_loss: 0.5259 - val_acc: 0.8355\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 382us/step - loss: 0.0344 - acc: 0.9947 - val_loss: 0.5939 - val_acc: 0.8315\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 359us/step - loss: 0.0205 - acc: 0.9980 - val_loss: 0.6338 - val_acc: 0.8335\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 357us/step - loss: 0.0123 - acc: 0.9989 - val_loss: 0.6934 - val_acc: 0.8307\n",
      "20000/20000 [==============================] - 6s 294us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 16s 970us/step - loss: 0.4056 - acc: 0.8196 - val_loss: 0.3509 - val_acc: 0.8482\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 354us/step - loss: 0.3085 - acc: 0.8686 - val_loss: 0.3435 - val_acc: 0.8530\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 361us/step - loss: 0.2614 - acc: 0.8924 - val_loss: 0.3636 - val_acc: 0.8440\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 364us/step - loss: 0.2091 - acc: 0.9179 - val_loss: 0.3763 - val_acc: 0.8403\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 397us/step - loss: 0.1501 - acc: 0.9481 - val_loss: 0.4084 - val_acc: 0.8460\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 398us/step - loss: 0.1009 - acc: 0.9705 - val_loss: 0.4570 - val_acc: 0.8363\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 386us/step - loss: 0.0607 - acc: 0.9875 - val_loss: 0.5354 - val_acc: 0.8377\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 393us/step - loss: 0.0363 - acc: 0.9944 - val_loss: 0.5852 - val_acc: 0.8365\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.0217 - acc: 0.9972 - val_loss: 0.6487 - val_acc: 0.8370\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 374us/step - loss: 0.0145 - acc: 0.9986 - val_loss: 0.6900 - val_acc: 0.8387\n",
      "20000/20000 [==============================] - 6s 295us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4050 - acc: 0.8224 - val_loss: 0.3549 - val_acc: 0.8448\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.3071 - acc: 0.8698 - val_loss: 0.3494 - val_acc: 0.8522\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 378us/step - loss: 0.2622 - acc: 0.8907 - val_loss: 0.3592 - val_acc: 0.8525\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 395us/step - loss: 0.2091 - acc: 0.9194 - val_loss: 0.3649 - val_acc: 0.8478\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 425us/step - loss: 0.1543 - acc: 0.9456 - val_loss: 0.4020 - val_acc: 0.8420\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 448us/step - loss: 0.1035 - acc: 0.9695 - val_loss: 0.4581 - val_acc: 0.8455\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 443us/step - loss: 0.0687 - acc: 0.9832 - val_loss: 0.4966 - val_acc: 0.8397\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 383us/step - loss: 0.0459 - acc: 0.9906 - val_loss: 0.5735 - val_acc: 0.8387\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 398us/step - loss: 0.0297 - acc: 0.9953 - val_loss: 0.6283 - val_acc: 0.8393\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.0199 - acc: 0.9968 - val_loss: 0.6594 - val_acc: 0.8347\n",
      "20000/20000 [==============================] - 6s 289us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4105 - acc: 0.8144 - val_loss: 0.3505 - val_acc: 0.8528\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 386us/step - loss: 0.3083 - acc: 0.8689 - val_loss: 0.3501 - val_acc: 0.8518\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.2660 - acc: 0.8891 - val_loss: 0.3524 - val_acc: 0.8550\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 381us/step - loss: 0.2184 - acc: 0.9159 - val_loss: 0.3742 - val_acc: 0.8532\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.1616 - acc: 0.9437 - val_loss: 0.4016 - val_acc: 0.8383\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 384us/step - loss: 0.1118 - acc: 0.9659 - val_loss: 0.4578 - val_acc: 0.8423\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 383us/step - loss: 0.0756 - acc: 0.9788 - val_loss: 0.4980 - val_acc: 0.8325\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.0502 - acc: 0.9889 - val_loss: 0.5601 - val_acc: 0.8317\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 392us/step - loss: 0.0345 - acc: 0.9933 - val_loss: 0.6043 - val_acc: 0.8345\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 381us/step - loss: 0.0243 - acc: 0.9961 - val_loss: 0.6742 - val_acc: 0.8343\n",
      "20000/20000 [==============================] - 6s 298us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4065 - acc: 0.8161 - val_loss: 0.3538 - val_acc: 0.8462\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 371us/step - loss: 0.3104 - acc: 0.8695 - val_loss: 0.3551 - val_acc: 0.8455\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 367us/step - loss: 0.2693 - acc: 0.8869 - val_loss: 0.3537 - val_acc: 0.8530\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 363us/step - loss: 0.2222 - acc: 0.9107 - val_loss: 0.3653 - val_acc: 0.8452\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 368us/step - loss: 0.1734 - acc: 0.9371 - val_loss: 0.4115 - val_acc: 0.8395\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 367us/step - loss: 0.1239 - acc: 0.9601 - val_loss: 0.4524 - val_acc: 0.8325\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 381us/step - loss: 0.0854 - acc: 0.9749 - val_loss: 0.5104 - val_acc: 0.8305\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 367us/step - loss: 0.0584 - acc: 0.9861 - val_loss: 0.5657 - val_acc: 0.8317\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 367us/step - loss: 0.0394 - acc: 0.9923 - val_loss: 0.6330 - val_acc: 0.8255\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 398us/step - loss: 0.0287 - acc: 0.9942 - val_loss: 0.6986 - val_acc: 0.8313\n",
      "20000/20000 [==============================] - 6s 321us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4120 - acc: 0.8149 - val_loss: 0.3514 - val_acc: 0.8475\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 431us/step - loss: 0.3105 - acc: 0.8665 - val_loss: 0.3451 - val_acc: 0.8515\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 401us/step - loss: 0.2670 - acc: 0.8902 - val_loss: 0.3524 - val_acc: 0.8505\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 384us/step - loss: 0.2238 - acc: 0.9120 - val_loss: 0.3666 - val_acc: 0.8502\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 352us/step - loss: 0.1744 - acc: 0.9371 - val_loss: 0.4003 - val_acc: 0.8448\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 359us/step - loss: 0.1276 - acc: 0.9593 - val_loss: 0.4290 - val_acc: 0.8375\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 352us/step - loss: 0.0920 - acc: 0.9728 - val_loss: 0.4842 - val_acc: 0.8330\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 353us/step - loss: 0.0631 - acc: 0.9841 - val_loss: 0.5358 - val_acc: 0.8377\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 353us/step - loss: 0.0462 - acc: 0.9892 - val_loss: 0.5956 - val_acc: 0.8323\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 384us/step - loss: 0.0330 - acc: 0.9929 - val_loss: 0.6223 - val_acc: 0.8337\n",
      "20000/20000 [==============================] - 6s 295us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 16s 982us/step - loss: 0.4119 - acc: 0.8153 - val_loss: 0.3505 - val_acc: 0.8520\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 5s 339us/step - loss: 0.3112 - acc: 0.8648 - val_loss: 0.3546 - val_acc: 0.8458\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 353us/step - loss: 0.2725 - acc: 0.8872 - val_loss: 0.3563 - val_acc: 0.8478\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 5s 341us/step - loss: 0.2273 - acc: 0.9087 - val_loss: 0.3815 - val_acc: 0.8498\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 353us/step - loss: 0.1810 - acc: 0.9336 - val_loss: 0.3955 - val_acc: 0.8450\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 355us/step - loss: 0.1368 - acc: 0.9541 - val_loss: 0.4266 - val_acc: 0.8347\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 393us/step - loss: 0.1003 - acc: 0.9671 - val_loss: 0.4874 - val_acc: 0.8327\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 351us/step - loss: 0.0711 - acc: 0.9799 - val_loss: 0.5247 - val_acc: 0.8340\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 349us/step - loss: 0.0518 - acc: 0.9861 - val_loss: 0.5916 - val_acc: 0.8377\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 361us/step - loss: 0.0417 - acc: 0.9893 - val_loss: 0.6548 - val_acc: 0.8330\n",
      "20000/20000 [==============================] - 6s 282us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 15s 935us/step - loss: 0.4080 - acc: 0.8186 - val_loss: 0.3512 - val_acc: 0.8480\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 364us/step - loss: 0.3095 - acc: 0.8699 - val_loss: 0.3506 - val_acc: 0.8498\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.2699 - acc: 0.8877 - val_loss: 0.3566 - val_acc: 0.8492\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 361us/step - loss: 0.2280 - acc: 0.9083 - val_loss: 0.3642 - val_acc: 0.8472\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 376us/step - loss: 0.1797 - acc: 0.9339 - val_loss: 0.3874 - val_acc: 0.8490\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 410us/step - loss: 0.1346 - acc: 0.9523 - val_loss: 0.4218 - val_acc: 0.8442\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 394us/step - loss: 0.1031 - acc: 0.9671 - val_loss: 0.4593 - val_acc: 0.8350\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 357us/step - loss: 0.0763 - acc: 0.9771 - val_loss: 0.5025 - val_acc: 0.8373\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.0545 - acc: 0.9862 - val_loss: 0.5667 - val_acc: 0.8367\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 363us/step - loss: 0.0442 - acc: 0.9881 - val_loss: 0.6204 - val_acc: 0.8333\n",
      "20000/20000 [==============================] - 6s 311us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 16s 973us/step - loss: 0.4116 - acc: 0.8140 - val_loss: 0.3519 - val_acc: 0.8495\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 382us/step - loss: 0.3116 - acc: 0.8653 - val_loss: 0.3443 - val_acc: 0.8490\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 357us/step - loss: 0.2720 - acc: 0.8846 - val_loss: 0.3584 - val_acc: 0.8505\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 355us/step - loss: 0.2305 - acc: 0.9104 - val_loss: 0.3743 - val_acc: 0.8468\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 376us/step - loss: 0.1842 - acc: 0.9326 - val_loss: 0.4035 - val_acc: 0.8417\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 356us/step - loss: 0.1442 - acc: 0.9492 - val_loss: 0.4259 - val_acc: 0.8373\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 353us/step - loss: 0.1072 - acc: 0.9650 - val_loss: 0.4805 - val_acc: 0.8367\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 358us/step - loss: 0.0788 - acc: 0.9759 - val_loss: 0.5148 - val_acc: 0.8340\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 355us/step - loss: 0.0618 - acc: 0.9826 - val_loss: 0.5609 - val_acc: 0.8370\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 360us/step - loss: 0.0468 - acc: 0.9876 - val_loss: 0.6102 - val_acc: 0.8333\n",
      "20000/20000 [==============================] - 6s 293us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 16s 993us/step - loss: 0.4156 - acc: 0.8124 - val_loss: 0.3548 - val_acc: 0.8458\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 360us/step - loss: 0.3181 - acc: 0.8641 - val_loss: 0.3509 - val_acc: 0.8500\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 357us/step - loss: 0.2798 - acc: 0.8816 - val_loss: 0.3581 - val_acc: 0.8455\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 358us/step - loss: 0.2440 - acc: 0.8986 - val_loss: 0.3673 - val_acc: 0.8468\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 356us/step - loss: 0.2043 - acc: 0.9195 - val_loss: 0.3890 - val_acc: 0.8510\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 364us/step - loss: 0.1609 - acc: 0.9401 - val_loss: 0.4169 - val_acc: 0.8367\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.1242 - acc: 0.9566 - val_loss: 0.4677 - val_acc: 0.8343\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 357us/step - loss: 0.0945 - acc: 0.9698 - val_loss: 0.5145 - val_acc: 0.8360\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 360us/step - loss: 0.0737 - acc: 0.9767 - val_loss: 0.5485 - val_acc: 0.8267\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 431us/step - loss: 0.0607 - acc: 0.9809 - val_loss: 0.5837 - val_acc: 0.8287\n",
      "20000/20000 [==============================] - 7s 361us/step\n",
      "1 9\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4063 - acc: 0.8204 - val_loss: 0.3517 - val_acc: 0.8470\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 380us/step - loss: 0.3113 - acc: 0.8664 - val_loss: 0.3547 - val_acc: 0.8458\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 419us/step - loss: 0.2673 - acc: 0.8880 - val_loss: 0.3553 - val_acc: 0.8495\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 369us/step - loss: 0.2137 - acc: 0.9164 - val_loss: 0.3767 - val_acc: 0.8452\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 367us/step - loss: 0.1533 - acc: 0.9491 - val_loss: 0.4119 - val_acc: 0.8393\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.1013 - acc: 0.9715 - val_loss: 0.4707 - val_acc: 0.8353\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.0630 - acc: 0.9853 - val_loss: 0.5205 - val_acc: 0.8357\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 347us/step - loss: 0.0369 - acc: 0.9939 - val_loss: 0.5721 - val_acc: 0.8317\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 349us/step - loss: 0.0212 - acc: 0.9981 - val_loss: 0.6303 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 348us/step - loss: 0.0125 - acc: 0.9994 - val_loss: 0.6860 - val_acc: 0.8315\n",
      "20000/20000 [==============================] - 6s 290us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4062 - acc: 0.8176 - val_loss: 0.3549 - val_acc: 0.8495\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 357us/step - loss: 0.3083 - acc: 0.8693 - val_loss: 0.3495 - val_acc: 0.8480\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 381us/step - loss: 0.2647 - acc: 0.8898 - val_loss: 0.3572 - val_acc: 0.8498\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 376us/step - loss: 0.2143 - acc: 0.9156 - val_loss: 0.3821 - val_acc: 0.8415\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 372us/step - loss: 0.1576 - acc: 0.9459 - val_loss: 0.4198 - val_acc: 0.8423\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 376us/step - loss: 0.1075 - acc: 0.9685 - val_loss: 0.4699 - val_acc: 0.8337\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 371us/step - loss: 0.0683 - acc: 0.9829 - val_loss: 0.5410 - val_acc: 0.8367\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 359us/step - loss: 0.0420 - acc: 0.9923 - val_loss: 0.6128 - val_acc: 0.8347\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 438us/step - loss: 0.0273 - acc: 0.9959 - val_loss: 0.6360 - val_acc: 0.8287\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 383us/step - loss: 0.0178 - acc: 0.9981 - val_loss: 0.7087 - val_acc: 0.8337\n",
      "20000/20000 [==============================] - 6s 305us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4069 - acc: 0.8189 - val_loss: 0.3527 - val_acc: 0.8450\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 384us/step - loss: 0.3082 - acc: 0.8668 - val_loss: 0.3463 - val_acc: 0.8515\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 366us/step - loss: 0.2662 - acc: 0.8905 - val_loss: 0.3549 - val_acc: 0.8485\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 357us/step - loss: 0.2171 - acc: 0.9138 - val_loss: 0.3676 - val_acc: 0.8448\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 5s 336us/step - loss: 0.1639 - acc: 0.9434 - val_loss: 0.4036 - val_acc: 0.8387\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 5s 337us/step - loss: 0.1162 - acc: 0.9638 - val_loss: 0.4570 - val_acc: 0.8410\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 5s 326us/step - loss: 0.0758 - acc: 0.9806 - val_loss: 0.5059 - val_acc: 0.8347\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 5s 325us/step - loss: 0.0515 - acc: 0.9890 - val_loss: 0.5711 - val_acc: 0.8313\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 5s 326us/step - loss: 0.0337 - acc: 0.9938 - val_loss: 0.6103 - val_acc: 0.8295\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 5s 328us/step - loss: 0.0249 - acc: 0.9960 - val_loss: 0.6720 - val_acc: 0.8303\n",
      "20000/20000 [==============================] - 5s 258us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 13s 833us/step - loss: 0.4067 - acc: 0.8157 - val_loss: 0.3483 - val_acc: 0.8485\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 5s 329us/step - loss: 0.3069 - acc: 0.8677 - val_loss: 0.3468 - val_acc: 0.8500\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 5s 334us/step - loss: 0.2626 - acc: 0.8888 - val_loss: 0.3517 - val_acc: 0.8522\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 5s 332us/step - loss: 0.2176 - acc: 0.9154 - val_loss: 0.3715 - val_acc: 0.8475\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 344us/step - loss: 0.1663 - acc: 0.9413 - val_loss: 0.4059 - val_acc: 0.8458\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 368us/step - loss: 0.1202 - acc: 0.9629 - val_loss: 0.4361 - val_acc: 0.8313\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 347us/step - loss: 0.0837 - acc: 0.9771 - val_loss: 0.4777 - val_acc: 0.8427\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 5s 343us/step - loss: 0.0563 - acc: 0.9858 - val_loss: 0.5222 - val_acc: 0.8370\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 346us/step - loss: 0.0365 - acc: 0.9926 - val_loss: 0.5670 - val_acc: 0.8393\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 366us/step - loss: 0.0260 - acc: 0.9951 - val_loss: 0.6331 - val_acc: 0.8383\n",
      "20000/20000 [==============================] - 6s 301us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4112 - acc: 0.8143 - val_loss: 0.3572 - val_acc: 0.8458\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 453us/step - loss: 0.3093 - acc: 0.8660 - val_loss: 0.3516 - val_acc: 0.8502\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 439us/step - loss: 0.2671 - acc: 0.8874 - val_loss: 0.3577 - val_acc: 0.8470\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 414us/step - loss: 0.2212 - acc: 0.9136 - val_loss: 0.3637 - val_acc: 0.8505\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 403us/step - loss: 0.1714 - acc: 0.9382 - val_loss: 0.4032 - val_acc: 0.8405\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 415us/step - loss: 0.1282 - acc: 0.9572 - val_loss: 0.4483 - val_acc: 0.8387\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 8s 514us/step - loss: 0.0908 - acc: 0.9739 - val_loss: 0.4930 - val_acc: 0.8410\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 456us/step - loss: 0.0609 - acc: 0.9843 - val_loss: 0.5621 - val_acc: 0.8350\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 8s 502us/step - loss: 0.0435 - acc: 0.9891 - val_loss: 0.6179 - val_acc: 0.8323\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 406us/step - loss: 0.0340 - acc: 0.9924 - val_loss: 0.6627 - val_acc: 0.8315\n",
      "20000/20000 [==============================] - 8s 376us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4107 - acc: 0.8187 - val_loss: 0.3616 - val_acc: 0.8452\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 435us/step - loss: 0.3107 - acc: 0.8681 - val_loss: 0.3469 - val_acc: 0.8520\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 403us/step - loss: 0.2689 - acc: 0.8857 - val_loss: 0.3568 - val_acc: 0.8482\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 443us/step - loss: 0.2268 - acc: 0.9103 - val_loss: 0.3684 - val_acc: 0.8430\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 406us/step - loss: 0.1779 - acc: 0.9336 - val_loss: 0.4132 - val_acc: 0.8397\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.1316 - acc: 0.9551 - val_loss: 0.4303 - val_acc: 0.8323\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 8s 485us/step - loss: 0.0972 - acc: 0.9696 - val_loss: 0.4813 - val_acc: 0.8340\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 408us/step - loss: 0.0674 - acc: 0.9817 - val_loss: 0.5446 - val_acc: 0.8317\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 395us/step - loss: 0.0498 - acc: 0.9872 - val_loss: 0.5760 - val_acc: 0.8267\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 405us/step - loss: 0.0385 - acc: 0.9906 - val_loss: 0.6492 - val_acc: 0.8280\n",
      "20000/20000 [==============================] - 7s 331us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4113 - acc: 0.8152 - val_loss: 0.3470 - val_acc: 0.8475\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 433us/step - loss: 0.3116 - acc: 0.8674 - val_loss: 0.3511 - val_acc: 0.8475\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 445us/step - loss: 0.2723 - acc: 0.8849 - val_loss: 0.3517 - val_acc: 0.8485\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 434us/step - loss: 0.2300 - acc: 0.9077 - val_loss: 0.3736 - val_acc: 0.8438\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 399us/step - loss: 0.1838 - acc: 0.9317 - val_loss: 0.3880 - val_acc: 0.8445\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 398us/step - loss: 0.1416 - acc: 0.9520 - val_loss: 0.4194 - val_acc: 0.8410\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.0998 - acc: 0.9686 - val_loss: 0.4780 - val_acc: 0.8410\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 413us/step - loss: 0.0732 - acc: 0.9791 - val_loss: 0.4994 - val_acc: 0.8277\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 435us/step - loss: 0.0577 - acc: 0.9844 - val_loss: 0.5777 - val_acc: 0.8317\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 8s 504us/step - loss: 0.0446 - acc: 0.9884 - val_loss: 0.6377 - val_acc: 0.8317\n",
      "20000/20000 [==============================] - 9s 435us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4125 - acc: 0.8110 - val_loss: 0.3547 - val_acc: 0.8448\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 364us/step - loss: 0.3113 - acc: 0.8656 - val_loss: 0.3491 - val_acc: 0.8440\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 370us/step - loss: 0.2723 - acc: 0.8863 - val_loss: 0.3577 - val_acc: 0.8495\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 376us/step - loss: 0.2294 - acc: 0.9073 - val_loss: 0.3719 - val_acc: 0.8470\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 393us/step - loss: 0.1859 - acc: 0.9298 - val_loss: 0.3922 - val_acc: 0.8400\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 384us/step - loss: 0.1406 - acc: 0.9506 - val_loss: 0.4342 - val_acc: 0.8413\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 376us/step - loss: 0.1053 - acc: 0.9646 - val_loss: 0.4618 - val_acc: 0.8385\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 387us/step - loss: 0.0810 - acc: 0.9756 - val_loss: 0.5121 - val_acc: 0.8353\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 386us/step - loss: 0.0582 - acc: 0.9837 - val_loss: 0.5507 - val_acc: 0.8405\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 374us/step - loss: 0.0451 - acc: 0.9884 - val_loss: 0.5855 - val_acc: 0.8387\n",
      "20000/20000 [==============================] - 6s 305us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4101 - acc: 0.8137 - val_loss: 0.3485 - val_acc: 0.8490\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 368us/step - loss: 0.3131 - acc: 0.8664 - val_loss: 0.3478 - val_acc: 0.8498\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 396us/step - loss: 0.2719 - acc: 0.8861 - val_loss: 0.3548 - val_acc: 0.8492\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.2317 - acc: 0.9045 - val_loss: 0.3700 - val_acc: 0.8500\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.1868 - acc: 0.9269 - val_loss: 0.3806 - val_acc: 0.8462\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 377us/step - loss: 0.1437 - acc: 0.9493 - val_loss: 0.4318 - val_acc: 0.8407\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 381us/step - loss: 0.1106 - acc: 0.9638 - val_loss: 0.4673 - val_acc: 0.8423\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 397us/step - loss: 0.0835 - acc: 0.9740 - val_loss: 0.5089 - val_acc: 0.8385\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 395us/step - loss: 0.0657 - acc: 0.9807 - val_loss: 0.5463 - val_acc: 0.8337\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 389us/step - loss: 0.0513 - acc: 0.9858 - val_loss: 0.5970 - val_acc: 0.8295\n",
      "20000/20000 [==============================] - 6s 316us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4115 - acc: 0.8171 - val_loss: 0.3502 - val_acc: 0.8488\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 377us/step - loss: 0.3146 - acc: 0.8649 - val_loss: 0.3493 - val_acc: 0.8545\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 380us/step - loss: 0.2777 - acc: 0.8837 - val_loss: 0.3630 - val_acc: 0.8472\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 399us/step - loss: 0.2425 - acc: 0.9001 - val_loss: 0.3745 - val_acc: 0.8475\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 380us/step - loss: 0.2055 - acc: 0.9213 - val_loss: 0.3879 - val_acc: 0.8423\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 383us/step - loss: 0.1605 - acc: 0.9428 - val_loss: 0.4158 - val_acc: 0.8435\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 386us/step - loss: 0.1266 - acc: 0.9559 - val_loss: 0.4524 - val_acc: 0.8370\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 380us/step - loss: 0.0949 - acc: 0.9711 - val_loss: 0.5149 - val_acc: 0.8340\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 381us/step - loss: 0.0778 - acc: 0.9768 - val_loss: 0.5469 - val_acc: 0.8285\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.0609 - acc: 0.9823 - val_loss: 0.6016 - val_acc: 0.8307\n",
      "20000/20000 [==============================] - 7s 328us/step\n",
      "2 9\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4079 - acc: 0.8157 - val_loss: 0.3524 - val_acc: 0.8522\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 389us/step - loss: 0.3062 - acc: 0.8707 - val_loss: 0.3548 - val_acc: 0.8512\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 373us/step - loss: 0.2612 - acc: 0.8898 - val_loss: 0.3569 - val_acc: 0.8452\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 371us/step - loss: 0.2092 - acc: 0.9173 - val_loss: 0.3683 - val_acc: 0.8448\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 395us/step - loss: 0.1517 - acc: 0.9476 - val_loss: 0.4102 - val_acc: 0.8407\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.1031 - acc: 0.9716 - val_loss: 0.4572 - val_acc: 0.8415\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.0634 - acc: 0.9864 - val_loss: 0.4952 - val_acc: 0.8370\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 369us/step - loss: 0.0398 - acc: 0.9937 - val_loss: 0.5575 - val_acc: 0.8410\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 368us/step - loss: 0.0239 - acc: 0.9969 - val_loss: 0.6197 - val_acc: 0.8397\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 364us/step - loss: 0.0149 - acc: 0.9991 - val_loss: 0.6715 - val_acc: 0.8387\n",
      "20000/20000 [==============================] - 6s 315us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4069 - acc: 0.8153 - val_loss: 0.3518 - val_acc: 0.8482\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 369us/step - loss: 0.3049 - acc: 0.8692 - val_loss: 0.3521 - val_acc: 0.8482\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 373us/step - loss: 0.2604 - acc: 0.8903 - val_loss: 0.3725 - val_acc: 0.8485\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 372us/step - loss: 0.2100 - acc: 0.9210 - val_loss: 0.3881 - val_acc: 0.8433\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 391us/step - loss: 0.1556 - acc: 0.9452 - val_loss: 0.4052 - val_acc: 0.8395\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 373us/step - loss: 0.1060 - acc: 0.9677 - val_loss: 0.4423 - val_acc: 0.8442\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 370us/step - loss: 0.0655 - acc: 0.9853 - val_loss: 0.5090 - val_acc: 0.8363\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 392us/step - loss: 0.0437 - acc: 0.9921 - val_loss: 0.5786 - val_acc: 0.8323\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 373us/step - loss: 0.0280 - acc: 0.9960 - val_loss: 0.6448 - val_acc: 0.8365\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 369us/step - loss: 0.0172 - acc: 0.9984 - val_loss: 0.6986 - val_acc: 0.8300\n",
      "20000/20000 [==============================] - 7s 340us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4084 - acc: 0.8134 - val_loss: 0.3577 - val_acc: 0.8413\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 377us/step - loss: 0.3078 - acc: 0.8666 - val_loss: 0.3527 - val_acc: 0.8462\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 381us/step - loss: 0.2618 - acc: 0.8886 - val_loss: 0.3678 - val_acc: 0.8435\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 377us/step - loss: 0.2136 - acc: 0.9171 - val_loss: 0.3774 - val_acc: 0.8485\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 383us/step - loss: 0.1619 - acc: 0.9447 - val_loss: 0.4148 - val_acc: 0.8377\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 389us/step - loss: 0.1123 - acc: 0.9663 - val_loss: 0.4485 - val_acc: 0.8375\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.0741 - acc: 0.9806 - val_loss: 0.5050 - val_acc: 0.8277\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 381us/step - loss: 0.0489 - acc: 0.9896 - val_loss: 0.5491 - val_acc: 0.8275\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 397us/step - loss: 0.0323 - acc: 0.9943 - val_loss: 0.5995 - val_acc: 0.8300\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 401us/step - loss: 0.0226 - acc: 0.9964 - val_loss: 0.6735 - val_acc: 0.8297\n",
      "20000/20000 [==============================] - 7s 327us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4117 - acc: 0.8129 - val_loss: 0.3451 - val_acc: 0.8512\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 389us/step - loss: 0.3099 - acc: 0.8693 - val_loss: 0.3480 - val_acc: 0.8542\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 393us/step - loss: 0.2662 - acc: 0.8863 - val_loss: 0.3510 - val_acc: 0.8505\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 402us/step - loss: 0.2222 - acc: 0.9114 - val_loss: 0.3696 - val_acc: 0.8488\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 398us/step - loss: 0.1689 - acc: 0.9396 - val_loss: 0.3908 - val_acc: 0.8440\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 403us/step - loss: 0.1220 - acc: 0.9604 - val_loss: 0.4396 - val_acc: 0.8400\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 393us/step - loss: 0.0837 - acc: 0.9766 - val_loss: 0.4817 - val_acc: 0.8385\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 391us/step - loss: 0.0577 - acc: 0.9863 - val_loss: 0.5338 - val_acc: 0.8367\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 413us/step - loss: 0.0400 - acc: 0.9914 - val_loss: 0.5962 - val_acc: 0.8320\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 414us/step - loss: 0.0295 - acc: 0.9944 - val_loss: 0.6390 - val_acc: 0.8355\n",
      "20000/20000 [==============================] - 7s 331us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4117 - acc: 0.8118 - val_loss: 0.3615 - val_acc: 0.8417\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 376us/step - loss: 0.3102 - acc: 0.8678 - val_loss: 0.3559 - val_acc: 0.8438\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 408us/step - loss: 0.2717 - acc: 0.8884 - val_loss: 0.3594 - val_acc: 0.8470\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 392us/step - loss: 0.2260 - acc: 0.9093 - val_loss: 0.3757 - val_acc: 0.8397\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 400us/step - loss: 0.1775 - acc: 0.9346 - val_loss: 0.4093 - val_acc: 0.8370\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 377us/step - loss: 0.1336 - acc: 0.9551 - val_loss: 0.4417 - val_acc: 0.8350\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 457us/step - loss: 0.0948 - acc: 0.9720 - val_loss: 0.4978 - val_acc: 0.8233\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 432us/step - loss: 0.0685 - acc: 0.9814 - val_loss: 0.5410 - val_acc: 0.8277\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 466us/step - loss: 0.0480 - acc: 0.9882 - val_loss: 0.6226 - val_acc: 0.8247\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 454us/step - loss: 0.0373 - acc: 0.9912 - val_loss: 0.6579 - val_acc: 0.8235\n",
      "20000/20000 [==============================] - 8s 423us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4075 - acc: 0.8164 - val_loss: 0.3502 - val_acc: 0.8472\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 464us/step - loss: 0.3138 - acc: 0.8671 - val_loss: 0.3428 - val_acc: 0.8488TA: 0s - loss: 0.3123 - acc\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 427us/step - loss: 0.2705 - acc: 0.8862 - val_loss: 0.3592 - val_acc: 0.8492\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 423us/step - loss: 0.2310 - acc: 0.9052 - val_loss: 0.3731 - val_acc: 0.8462\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 423us/step - loss: 0.1844 - acc: 0.9297 - val_loss: 0.3975 - val_acc: 0.8390\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 430us/step - loss: 0.1427 - acc: 0.9505 - val_loss: 0.4280 - val_acc: 0.8405\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 421us/step - loss: 0.1015 - acc: 0.9690 - val_loss: 0.4791 - val_acc: 0.8383\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 426us/step - loss: 0.0740 - acc: 0.9785 - val_loss: 0.5579 - val_acc: 0.8307\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 429us/step - loss: 0.0559 - acc: 0.9846 - val_loss: 0.6073 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 452us/step - loss: 0.0436 - acc: 0.9886 - val_loss: 0.6407 - val_acc: 0.8233\n",
      "20000/20000 [==============================] - 7s 357us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4155 - acc: 0.8117 - val_loss: 0.3500 - val_acc: 0.8462\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 417us/step - loss: 0.3118 - acc: 0.8661 - val_loss: 0.3510 - val_acc: 0.8498\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 411us/step - loss: 0.2726 - acc: 0.8838 - val_loss: 0.3540 - val_acc: 0.8518\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.2329 - acc: 0.9066 - val_loss: 0.3627 - val_acc: 0.8468\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 375us/step - loss: 0.1852 - acc: 0.9296 - val_loss: 0.3903 - val_acc: 0.8430\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 376us/step - loss: 0.1435 - acc: 0.9489 - val_loss: 0.4309 - val_acc: 0.8413\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 375us/step - loss: 0.1053 - acc: 0.9661 - val_loss: 0.4682 - val_acc: 0.8393\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 376us/step - loss: 0.0772 - acc: 0.9769 - val_loss: 0.5004 - val_acc: 0.8315\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 392us/step - loss: 0.0570 - acc: 0.9859 - val_loss: 0.5529 - val_acc: 0.8343\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 377us/step - loss: 0.0435 - acc: 0.9897 - val_loss: 0.6078 - val_acc: 0.8343\n",
      "20000/20000 [==============================] - 6s 322us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4113 - acc: 0.8192 - val_loss: 0.3516 - val_acc: 0.8505\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 374us/step - loss: 0.3121 - acc: 0.8644 - val_loss: 0.3487 - val_acc: 0.8512\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 372us/step - loss: 0.2722 - acc: 0.8849 - val_loss: 0.3601 - val_acc: 0.8518\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 373us/step - loss: 0.2320 - acc: 0.9054 - val_loss: 0.3716 - val_acc: 0.8488\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 377us/step - loss: 0.1876 - acc: 0.9287 - val_loss: 0.3952 - val_acc: 0.8490\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 372us/step - loss: 0.1459 - acc: 0.9461 - val_loss: 0.4251 - val_acc: 0.8423\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 375us/step - loss: 0.1052 - acc: 0.9668 - val_loss: 0.4781 - val_acc: 0.8383\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 387us/step - loss: 0.0817 - acc: 0.9762 - val_loss: 0.5220 - val_acc: 0.8420\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 374us/step - loss: 0.0630 - acc: 0.9827 - val_loss: 0.5869 - val_acc: 0.8367\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 371us/step - loss: 0.0500 - acc: 0.9864 - val_loss: 0.6171 - val_acc: 0.8363\n",
      "20000/20000 [==============================] - 6s 315us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4135 - acc: 0.8152 - val_loss: 0.3470 - val_acc: 0.8498\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 369us/step - loss: 0.3111 - acc: 0.8655 - val_loss: 0.3484 - val_acc: 0.8552\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 386us/step - loss: 0.2727 - acc: 0.8844 - val_loss: 0.3530 - val_acc: 0.8495\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 383us/step - loss: 0.2348 - acc: 0.9049 - val_loss: 0.3713 - val_acc: 0.8462\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 396us/step - loss: 0.1917 - acc: 0.9259 - val_loss: 0.3869 - val_acc: 0.8413\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 380us/step - loss: 0.1513 - acc: 0.9464 - val_loss: 0.4257 - val_acc: 0.8367\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 381us/step - loss: 0.1189 - acc: 0.9601 - val_loss: 0.4688 - val_acc: 0.8357\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 387us/step - loss: 0.0883 - acc: 0.9728 - val_loss: 0.5110 - val_acc: 0.8355\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 383us/step - loss: 0.0698 - acc: 0.9788 - val_loss: 0.5581 - val_acc: 0.8310\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 380us/step - loss: 0.0563 - acc: 0.9844 - val_loss: 0.5915 - val_acc: 0.8293\n",
      "20000/20000 [==============================] - 6s 322us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4165 - acc: 0.8101 - val_loss: 0.3516 - val_acc: 0.8525\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.3164 - acc: 0.8653 - val_loss: 0.3526 - val_acc: 0.8488\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 399us/step - loss: 0.2798 - acc: 0.8815 - val_loss: 0.3627 - val_acc: 0.8470\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 386us/step - loss: 0.2451 - acc: 0.8987 - val_loss: 0.3720 - val_acc: 0.8510\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 383us/step - loss: 0.2040 - acc: 0.9219 - val_loss: 0.3862 - val_acc: 0.8495\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 391us/step - loss: 0.1641 - acc: 0.9419 - val_loss: 0.4203 - val_acc: 0.8433\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 396us/step - loss: 0.1294 - acc: 0.9549 - val_loss: 0.4291 - val_acc: 0.8395\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 395us/step - loss: 0.0978 - acc: 0.9688 - val_loss: 0.4814 - val_acc: 0.8377\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.0764 - acc: 0.9764 - val_loss: 0.5416 - val_acc: 0.8375\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 391us/step - loss: 0.0587 - acc: 0.9823 - val_loss: 0.5709 - val_acc: 0.8365\n",
      "20000/20000 [==============================] - 6s 314us/step\n",
      "3 9\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4079 - acc: 0.8195 - val_loss: 0.3536 - val_acc: 0.8470\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 381us/step - loss: 0.3108 - acc: 0.8658 - val_loss: 0.3480 - val_acc: 0.8535\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 368us/step - loss: 0.2625 - acc: 0.8914 - val_loss: 0.3635 - val_acc: 0.8470\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 375us/step - loss: 0.2141 - acc: 0.9159 - val_loss: 0.3725 - val_acc: 0.8430\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 370us/step - loss: 0.1594 - acc: 0.9443 - val_loss: 0.4195 - val_acc: 0.8407\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 377us/step - loss: 0.1077 - acc: 0.9674 - val_loss: 0.4483 - val_acc: 0.8390\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.0721 - acc: 0.9815 - val_loss: 0.4993 - val_acc: 0.8347\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 372us/step - loss: 0.0451 - acc: 0.9912 - val_loss: 0.5622 - val_acc: 0.8343\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 372us/step - loss: 0.0279 - acc: 0.9962 - val_loss: 0.6211 - val_acc: 0.8295\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 389us/step - loss: 0.0182 - acc: 0.9982 - val_loss: 0.6757 - val_acc: 0.8357\n",
      "20000/20000 [==============================] - 7s 331us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4089 - acc: 0.8141 - val_loss: 0.3499 - val_acc: 0.8508\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.3082 - acc: 0.8710 - val_loss: 0.3494 - val_acc: 0.8480\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.2659 - acc: 0.8884 - val_loss: 0.3527 - val_acc: 0.8502\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.2173 - acc: 0.9139 - val_loss: 0.3800 - val_acc: 0.8452\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.1646 - acc: 0.9420 - val_loss: 0.4109 - val_acc: 0.8423\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 402us/step - loss: 0.1163 - acc: 0.9628 - val_loss: 0.4547 - val_acc: 0.8337\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.0767 - acc: 0.9796 - val_loss: 0.5043 - val_acc: 0.8375\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.0495 - acc: 0.9898 - val_loss: 0.5602 - val_acc: 0.8300\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 387us/step - loss: 0.0325 - acc: 0.9949 - val_loss: 0.6294 - val_acc: 0.8327\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.0218 - acc: 0.9968 - val_loss: 0.6510 - val_acc: 0.8317\n",
      "20000/20000 [==============================] - 6s 313us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4083 - acc: 0.8176 - val_loss: 0.3516 - val_acc: 0.8518\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 393us/step - loss: 0.3123 - acc: 0.8654 - val_loss: 0.3484 - val_acc: 0.8482\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 396us/step - loss: 0.2708 - acc: 0.8855 - val_loss: 0.3562 - val_acc: 0.8462\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 400us/step - loss: 0.2260 - acc: 0.9103 - val_loss: 0.3663 - val_acc: 0.8423\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.1740 - acc: 0.9378 - val_loss: 0.3936 - val_acc: 0.8423\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.1262 - acc: 0.9584 - val_loss: 0.4339 - val_acc: 0.8375\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 386us/step - loss: 0.0855 - acc: 0.9762 - val_loss: 0.5005 - val_acc: 0.8387\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 397us/step - loss: 0.0577 - acc: 0.9862 - val_loss: 0.5443 - val_acc: 0.8390\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 445us/step - loss: 0.0402 - acc: 0.9922 - val_loss: 0.6030 - val_acc: 0.8335\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 391us/step - loss: 0.0296 - acc: 0.9944 - val_loss: 0.6422 - val_acc: 0.8297\n",
      "20000/20000 [==============================] - 7s 337us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4127 - acc: 0.8121 - val_loss: 0.3494 - val_acc: 0.8478\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 386us/step - loss: 0.3131 - acc: 0.8653 - val_loss: 0.3455 - val_acc: 0.8490\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 393us/step - loss: 0.2737 - acc: 0.8858 - val_loss: 0.3509 - val_acc: 0.8488\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 391us/step - loss: 0.2304 - acc: 0.9041 - val_loss: 0.3681 - val_acc: 0.8462\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.1817 - acc: 0.9314 - val_loss: 0.3819 - val_acc: 0.8440\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 393us/step - loss: 0.1327 - acc: 0.9553 - val_loss: 0.4273 - val_acc: 0.8420\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.0933 - acc: 0.9729 - val_loss: 0.4829 - val_acc: 0.8377\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 405us/step - loss: 0.0659 - acc: 0.9825 - val_loss: 0.5277 - val_acc: 0.8355\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 398us/step - loss: 0.0470 - acc: 0.9903 - val_loss: 0.6120 - val_acc: 0.8407\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 395us/step - loss: 0.0374 - acc: 0.9916 - val_loss: 0.6475 - val_acc: 0.8353\n",
      "20000/20000 [==============================] - 7s 334us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4098 - acc: 0.8177 - val_loss: 0.3520 - val_acc: 0.8455\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 396us/step - loss: 0.3129 - acc: 0.8668 - val_loss: 0.3459 - val_acc: 0.8548\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 397us/step - loss: 0.2737 - acc: 0.8864 - val_loss: 0.3560 - val_acc: 0.8492\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 395us/step - loss: 0.2352 - acc: 0.9053 - val_loss: 0.3665 - val_acc: 0.8492\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 396us/step - loss: 0.1884 - acc: 0.9282 - val_loss: 0.3915 - val_acc: 0.8472\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 396us/step - loss: 0.1439 - acc: 0.9476 - val_loss: 0.4268 - val_acc: 0.8433\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 413us/step - loss: 0.1047 - acc: 0.9652 - val_loss: 0.4708 - val_acc: 0.8385\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 410us/step - loss: 0.0749 - acc: 0.9802 - val_loss: 0.5167 - val_acc: 0.8353\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 404us/step - loss: 0.0527 - acc: 0.9866 - val_loss: 0.5899 - val_acc: 0.8377\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 404us/step - loss: 0.0372 - acc: 0.9919 - val_loss: 0.6247 - val_acc: 0.8313\n",
      "20000/20000 [==============================] - 7s 339us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4113 - acc: 0.8163 - val_loss: 0.3495 - val_acc: 0.8472\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 401us/step - loss: 0.3124 - acc: 0.8638 - val_loss: 0.3487 - val_acc: 0.8465\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 406us/step - loss: 0.2735 - acc: 0.8854 - val_loss: 0.3572 - val_acc: 0.8528\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 396us/step - loss: 0.2342 - acc: 0.9051 - val_loss: 0.3634 - val_acc: 0.8500\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 407us/step - loss: 0.1867 - acc: 0.9313 - val_loss: 0.3945 - val_acc: 0.8448\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 402us/step - loss: 0.1418 - acc: 0.9514 - val_loss: 0.4414 - val_acc: 0.8417\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 391us/step - loss: 0.1046 - acc: 0.9673 - val_loss: 0.4658 - val_acc: 0.8355\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 412us/step - loss: 0.0763 - acc: 0.9772 - val_loss: 0.5157 - val_acc: 0.8335\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 401us/step - loss: 0.0543 - acc: 0.9866 - val_loss: 0.5629 - val_acc: 0.8415\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 401us/step - loss: 0.0424 - acc: 0.9894 - val_loss: 0.6124 - val_acc: 0.8357\n",
      "20000/20000 [==============================] - 6s 320us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4112 - acc: 0.8128 - val_loss: 0.3532 - val_acc: 0.8452\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 387us/step - loss: 0.3137 - acc: 0.8667 - val_loss: 0.3527 - val_acc: 0.8520\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 422us/step - loss: 0.2758 - acc: 0.8847 - val_loss: 0.3593 - val_acc: 0.8505\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 410us/step - loss: 0.2324 - acc: 0.9048 - val_loss: 0.3662 - val_acc: 0.8498\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 386us/step - loss: 0.1887 - acc: 0.9289 - val_loss: 0.3861 - val_acc: 0.8445\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 389us/step - loss: 0.1428 - acc: 0.9505 - val_loss: 0.4322 - val_acc: 0.8373\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 378us/step - loss: 0.1033 - acc: 0.9666 - val_loss: 0.4723 - val_acc: 0.8445\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.0760 - acc: 0.9783 - val_loss: 0.5046 - val_acc: 0.8357\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 378us/step - loss: 0.0585 - acc: 0.9830 - val_loss: 0.5976 - val_acc: 0.8357\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 382us/step - loss: 0.0478 - acc: 0.9870 - val_loss: 0.6196 - val_acc: 0.8303\n",
      "20000/20000 [==============================] - 6s 317us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4149 - acc: 0.8104 - val_loss: 0.3520 - val_acc: 0.8535\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 366us/step - loss: 0.3119 - acc: 0.8659 - val_loss: 0.3521 - val_acc: 0.8485\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 365us/step - loss: 0.2732 - acc: 0.8862 - val_loss: 0.3562 - val_acc: 0.8460\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 371us/step - loss: 0.2331 - acc: 0.9042 - val_loss: 0.3734 - val_acc: 0.8470\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.1912 - acc: 0.9256 - val_loss: 0.3862 - val_acc: 0.8440\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 378us/step - loss: 0.1495 - acc: 0.9475 - val_loss: 0.4214 - val_acc: 0.8417\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 401us/step - loss: 0.1125 - acc: 0.9630 - val_loss: 0.4608 - val_acc: 0.8330\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 377us/step - loss: 0.0843 - acc: 0.9738 - val_loss: 0.5123 - val_acc: 0.8357\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.0680 - acc: 0.9798 - val_loss: 0.5495 - val_acc: 0.8333\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 376us/step - loss: 0.0524 - acc: 0.9848 - val_loss: 0.5937 - val_acc: 0.8330\n",
      "20000/20000 [==============================] - 6s 319us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4131 - acc: 0.8138 - val_loss: 0.3483 - val_acc: 0.8478\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 417us/step - loss: 0.3122 - acc: 0.8674 - val_loss: 0.3524 - val_acc: 0.8488\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 383us/step - loss: 0.2762 - acc: 0.8839 - val_loss: 0.3607 - val_acc: 0.8520\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 384us/step - loss: 0.2412 - acc: 0.9014 - val_loss: 0.3743 - val_acc: 0.8448\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 387us/step - loss: 0.2005 - acc: 0.9233 - val_loss: 0.3861 - val_acc: 0.8433\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 382us/step - loss: 0.1574 - acc: 0.9423 - val_loss: 0.4080 - val_acc: 0.8430\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 383us/step - loss: 0.1211 - acc: 0.9584 - val_loss: 0.4428 - val_acc: 0.8390\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 406us/step - loss: 0.0932 - acc: 0.9699 - val_loss: 0.4920 - val_acc: 0.8333\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.0754 - acc: 0.9753 - val_loss: 0.5265 - val_acc: 0.8367\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 384us/step - loss: 0.0636 - acc: 0.9791 - val_loss: 0.5358 - val_acc: 0.8323\n",
      "20000/20000 [==============================] - 6s 322us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4138 - acc: 0.8098 - val_loss: 0.3462 - val_acc: 0.8528\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 404us/step - loss: 0.3171 - acc: 0.8637 - val_loss: 0.3500 - val_acc: 0.8498\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 424us/step - loss: 0.2805 - acc: 0.8810 - val_loss: 0.3622 - val_acc: 0.8472\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 402us/step - loss: 0.2487 - acc: 0.8974 - val_loss: 0.3590 - val_acc: 0.8532\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 401us/step - loss: 0.2099 - acc: 0.9193 - val_loss: 0.3867 - val_acc: 0.8445\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 403us/step - loss: 0.1731 - acc: 0.9357 - val_loss: 0.4027 - val_acc: 0.8400\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 426us/step - loss: 0.1376 - acc: 0.9526 - val_loss: 0.4413 - val_acc: 0.8390\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 397us/step - loss: 0.1083 - acc: 0.9637 - val_loss: 0.4762 - val_acc: 0.8337\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 397us/step - loss: 0.0829 - acc: 0.9724 - val_loss: 0.5244 - val_acc: 0.8320\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 397us/step - loss: 0.0652 - acc: 0.9811 - val_loss: 0.5679 - val_acc: 0.8320\n",
      "20000/20000 [==============================] - 7s 330us/step\n",
      "4 9\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4087 - acc: 0.8180 - val_loss: 0.3546 - val_acc: 0.8470\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 384us/step - loss: 0.3110 - acc: 0.8677 - val_loss: 0.3541 - val_acc: 0.8495\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 393us/step - loss: 0.2679 - acc: 0.8871 - val_loss: 0.3529 - val_acc: 0.8535\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.2190 - acc: 0.9119 - val_loss: 0.3705 - val_acc: 0.8458\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 392us/step - loss: 0.1652 - acc: 0.9410 - val_loss: 0.4023 - val_acc: 0.8460\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 410us/step - loss: 0.1132 - acc: 0.9664 - val_loss: 0.4478 - val_acc: 0.8400\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 393us/step - loss: 0.0729 - acc: 0.9828 - val_loss: 0.5115 - val_acc: 0.8335\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.0454 - acc: 0.9915 - val_loss: 0.5629 - val_acc: 0.8295\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.0296 - acc: 0.9958 - val_loss: 0.6120 - val_acc: 0.8320\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 399us/step - loss: 0.0183 - acc: 0.9978 - val_loss: 0.6750 - val_acc: 0.8293\n",
      "20000/20000 [==============================] - 7s 333us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4115 - acc: 0.8151 - val_loss: 0.3557 - val_acc: 0.8472\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 401us/step - loss: 0.3115 - acc: 0.8674 - val_loss: 0.3509 - val_acc: 0.8478\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 421us/step - loss: 0.2735 - acc: 0.8847 - val_loss: 0.3561 - val_acc: 0.8488\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 401us/step - loss: 0.2269 - acc: 0.9087 - val_loss: 0.3707 - val_acc: 0.8488\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 400us/step - loss: 0.1761 - acc: 0.9338 - val_loss: 0.4090 - val_acc: 0.8460\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 414us/step - loss: 0.1267 - acc: 0.9591 - val_loss: 0.4345 - val_acc: 0.8395\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 400us/step - loss: 0.0860 - acc: 0.9763 - val_loss: 0.5109 - val_acc: 0.8317\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 396us/step - loss: 0.0589 - acc: 0.9850 - val_loss: 0.5605 - val_acc: 0.8325\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 403us/step - loss: 0.0395 - acc: 0.9925 - val_loss: 0.6035 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 412us/step - loss: 0.0243 - acc: 0.9964 - val_loss: 0.6580 - val_acc: 0.8285\n",
      "20000/20000 [==============================] - 7s 353us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4129 - acc: 0.8119 - val_loss: 0.3543 - val_acc: 0.8465\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 464us/step - loss: 0.3137 - acc: 0.8642 - val_loss: 0.3495 - val_acc: 0.8542\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 429us/step - loss: 0.2720 - acc: 0.8849 - val_loss: 0.3574 - val_acc: 0.8515\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 418us/step - loss: 0.2278 - acc: 0.9088 - val_loss: 0.3683 - val_acc: 0.8450\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 453us/step - loss: 0.1781 - acc: 0.9345 - val_loss: 0.3970 - val_acc: 0.8423\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 9s 541us/step - loss: 0.1288 - acc: 0.9576 - val_loss: 0.4452 - val_acc: 0.8370\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 465us/step - loss: 0.0898 - acc: 0.9746 - val_loss: 0.4957 - val_acc: 0.8313\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 431us/step - loss: 0.0595 - acc: 0.9846 - val_loss: 0.5502 - val_acc: 0.8377\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 463us/step - loss: 0.0407 - acc: 0.9914 - val_loss: 0.6231 - val_acc: 0.8313\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 463us/step - loss: 0.0299 - acc: 0.9942 - val_loss: 0.6729 - val_acc: 0.8313\n",
      "20000/20000 [==============================] - 7s 362us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4120 - acc: 0.8146 - val_loss: 0.3487 - val_acc: 0.8472\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 399us/step - loss: 0.3138 - acc: 0.8649 - val_loss: 0.3460 - val_acc: 0.8532\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 403us/step - loss: 0.2746 - acc: 0.8827 - val_loss: 0.3554 - val_acc: 0.8555\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 430us/step - loss: 0.2337 - acc: 0.9059 - val_loss: 0.3687 - val_acc: 0.8495\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 413us/step - loss: 0.1834 - acc: 0.9324 - val_loss: 0.3896 - val_acc: 0.8450\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 400us/step - loss: 0.1353 - acc: 0.9557 - val_loss: 0.4375 - val_acc: 0.8435\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 413us/step - loss: 0.0962 - acc: 0.9712 - val_loss: 0.4780 - val_acc: 0.8417\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 415us/step - loss: 0.0686 - acc: 0.9806 - val_loss: 0.5467 - val_acc: 0.8335\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 417us/step - loss: 0.0454 - acc: 0.9896 - val_loss: 0.5889 - val_acc: 0.8327\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 410us/step - loss: 0.0339 - acc: 0.9922 - val_loss: 0.6422 - val_acc: 0.8257\n",
      "20000/20000 [==============================] - 7s 357us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4106 - acc: 0.8172 - val_loss: 0.3510 - val_acc: 0.8452\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.3081 - acc: 0.8660 - val_loss: 0.3535 - val_acc: 0.8492\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 409us/step - loss: 0.2707 - acc: 0.8871 - val_loss: 0.3563 - val_acc: 0.8530\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 401us/step - loss: 0.2268 - acc: 0.9073 - val_loss: 0.3682 - val_acc: 0.8478\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 402us/step - loss: 0.1793 - acc: 0.9332 - val_loss: 0.3980 - val_acc: 0.8512\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 392us/step - loss: 0.1329 - acc: 0.9545 - val_loss: 0.4290 - val_acc: 0.8435\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 404us/step - loss: 0.0987 - acc: 0.9691 - val_loss: 0.4560 - val_acc: 0.8393\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 409us/step - loss: 0.0698 - acc: 0.9802 - val_loss: 0.5266 - val_acc: 0.8367\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 8s 505us/step - loss: 0.0507 - acc: 0.9881 - val_loss: 0.5820 - val_acc: 0.8337\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 433us/step - loss: 0.0380 - acc: 0.9911 - val_loss: 0.6428 - val_acc: 0.8313\n",
      "20000/20000 [==============================] - 7s 348us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4153 - acc: 0.8109 - val_loss: 0.3438 - val_acc: 0.8495\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 403us/step - loss: 0.3172 - acc: 0.8645 - val_loss: 0.3462 - val_acc: 0.8485\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 450us/step - loss: 0.2808 - acc: 0.8814 - val_loss: 0.3552 - val_acc: 0.8488\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 451us/step - loss: 0.2444 - acc: 0.8992 - val_loss: 0.3632 - val_acc: 0.8478\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 421us/step - loss: 0.2027 - acc: 0.9233 - val_loss: 0.3828 - val_acc: 0.8450\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 432us/step - loss: 0.1591 - acc: 0.9430 - val_loss: 0.4237 - val_acc: 0.8390\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 438us/step - loss: 0.1180 - acc: 0.9611 - val_loss: 0.4696 - val_acc: 0.8395\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 422us/step - loss: 0.0864 - acc: 0.9741 - val_loss: 0.5164 - val_acc: 0.8317\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 448us/step - loss: 0.0641 - acc: 0.9830 - val_loss: 0.5508 - val_acc: 0.8300\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 417us/step - loss: 0.0493 - acc: 0.9870 - val_loss: 0.5902 - val_acc: 0.8350\n",
      "20000/20000 [==============================] - 7s 347us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4118 - acc: 0.8132 - val_loss: 0.3537 - val_acc: 0.8448\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 404us/step - loss: 0.3165 - acc: 0.8640 - val_loss: 0.3512 - val_acc: 0.8470\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 403us/step - loss: 0.2782 - acc: 0.8807 - val_loss: 0.3608 - val_acc: 0.8480\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 406us/step - loss: 0.2423 - acc: 0.9011 - val_loss: 0.3712 - val_acc: 0.8502\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 467us/step - loss: 0.2019 - acc: 0.9216 - val_loss: 0.3880 - val_acc: 0.8405\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 410us/step - loss: 0.1564 - acc: 0.9422 - val_loss: 0.4156 - val_acc: 0.8377\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 411us/step - loss: 0.1187 - acc: 0.9605 - val_loss: 0.4420 - val_acc: 0.8380\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 421us/step - loss: 0.0907 - acc: 0.9709 - val_loss: 0.5052 - val_acc: 0.8305\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 450us/step - loss: 0.0700 - acc: 0.9790 - val_loss: 0.5383 - val_acc: 0.8287\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 425us/step - loss: 0.0544 - acc: 0.9854 - val_loss: 0.6022 - val_acc: 0.8317\n",
      "20000/20000 [==============================] - 7s 341us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4090 - acc: 0.8159 - val_loss: 0.3473 - val_acc: 0.8480\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 400us/step - loss: 0.3123 - acc: 0.8670 - val_loss: 0.3485 - val_acc: 0.8550\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 415us/step - loss: 0.2746 - acc: 0.8816 - val_loss: 0.3540 - val_acc: 0.8542\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 419us/step - loss: 0.2358 - acc: 0.9032 - val_loss: 0.3587 - val_acc: 0.8482\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 418us/step - loss: 0.1960 - acc: 0.9243 - val_loss: 0.4043 - val_acc: 0.8452\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 404us/step - loss: 0.1536 - acc: 0.9459 - val_loss: 0.4241 - val_acc: 0.8410\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 428us/step - loss: 0.1158 - acc: 0.9614 - val_loss: 0.4714 - val_acc: 0.8380\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 420us/step - loss: 0.0909 - acc: 0.9720 - val_loss: 0.5186 - val_acc: 0.8425\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 408us/step - loss: 0.0719 - acc: 0.9784 - val_loss: 0.5593 - val_acc: 0.8363\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 404us/step - loss: 0.0583 - acc: 0.9827 - val_loss: 0.6171 - val_acc: 0.8355\n",
      "20000/20000 [==============================] - 7s 345us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4172 - acc: 0.8109 - val_loss: 0.3489 - val_acc: 0.8492\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 413us/step - loss: 0.3160 - acc: 0.8619 - val_loss: 0.3505 - val_acc: 0.8452\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 414us/step - loss: 0.2791 - acc: 0.8813 - val_loss: 0.3552 - val_acc: 0.8470\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 416us/step - loss: 0.2446 - acc: 0.8963 - val_loss: 0.3652 - val_acc: 0.8460\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 425us/step - loss: 0.2052 - acc: 0.9206 - val_loss: 0.3877 - val_acc: 0.8450\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 447us/step - loss: 0.1631 - acc: 0.9410 - val_loss: 0.4109 - val_acc: 0.8357\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 454us/step - loss: 0.1279 - acc: 0.9554 - val_loss: 0.4642 - val_acc: 0.8347\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 463us/step - loss: 0.1015 - acc: 0.9671 - val_loss: 0.4935 - val_acc: 0.8343\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 438us/step - loss: 0.0813 - acc: 0.9738 - val_loss: 0.5367 - val_acc: 0.8343\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 8s 472us/step - loss: 0.0625 - acc: 0.9801 - val_loss: 0.5855 - val_acc: 0.8355\n",
      "20000/20000 [==============================] - 7s 332us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.4153 - acc: 0.8119 - val_loss: 0.3525 - val_acc: 0.8495\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 8s 498us/step - loss: 0.3173 - acc: 0.8647 - val_loss: 0.3466 - val_acc: 0.8482\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 438us/step - loss: 0.2808 - acc: 0.8794 - val_loss: 0.3608 - val_acc: 0.8468\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 8s 502us/step - loss: 0.2481 - acc: 0.8959 - val_loss: 0.3707 - val_acc: 0.8485\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 397us/step - loss: 0.2077 - acc: 0.9174 - val_loss: 0.3790 - val_acc: 0.8450\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 399us/step - loss: 0.1693 - acc: 0.9367 - val_loss: 0.4061 - val_acc: 0.8427\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 393us/step - loss: 0.1332 - acc: 0.9532 - val_loss: 0.4368 - val_acc: 0.8353\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 394us/step - loss: 0.1077 - acc: 0.9643 - val_loss: 0.4906 - val_acc: 0.8323\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 393us/step - loss: 0.0834 - acc: 0.9728 - val_loss: 0.5319 - val_acc: 0.8313\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 434us/step - loss: 0.0676 - acc: 0.9800 - val_loss: 0.5877 - val_acc: 0.8327\n",
      "20000/20000 [==============================] - 7s 338us/step\n",
      "5 9\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4133 - acc: 0.8125 - val_loss: 0.3519 - val_acc: 0.8512\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 426us/step - loss: 0.3161 - acc: 0.8657 - val_loss: 0.3470 - val_acc: 0.8552\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 422us/step - loss: 0.2754 - acc: 0.8821 - val_loss: 0.3594 - val_acc: 0.8480 ETA:\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 432us/step - loss: 0.2314 - acc: 0.9073 - val_loss: 0.3649 - val_acc: 0.8482\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 405us/step - loss: 0.1818 - acc: 0.9331 - val_loss: 0.3959 - val_acc: 0.8455\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 422us/step - loss: 0.1336 - acc: 0.9552 - val_loss: 0.4263 - val_acc: 0.8367\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 455us/step - loss: 0.0913 - acc: 0.9744 - val_loss: 0.4844 - val_acc: 0.8430\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 429us/step - loss: 0.0594 - acc: 0.9859 - val_loss: 0.5411 - val_acc: 0.8340\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 449us/step - loss: 0.0397 - acc: 0.9921 - val_loss: 0.6138 - val_acc: 0.8373\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 455us/step - loss: 0.0259 - acc: 0.9959 - val_loss: 0.6680 - val_acc: 0.8350\n",
      "20000/20000 [==============================] - 7s 356us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.4102 - acc: 0.8158 - val_loss: 0.3572 - val_acc: 0.8472\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 446us/step - loss: 0.3128 - acc: 0.8668 - val_loss: 0.3489 - val_acc: 0.8508\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 440us/step - loss: 0.2747 - acc: 0.8831 - val_loss: 0.3560 - val_acc: 0.8482\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 458us/step - loss: 0.2332 - acc: 0.9066 - val_loss: 0.3632 - val_acc: 0.8545\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 448us/step - loss: 0.1833 - acc: 0.9313 - val_loss: 0.3933 - val_acc: 0.8420\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 428us/step - loss: 0.1326 - acc: 0.9555 - val_loss: 0.4300 - val_acc: 0.8357\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 424us/step - loss: 0.0927 - acc: 0.9726 - val_loss: 0.4775 - val_acc: 0.8343\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 446us/step - loss: 0.0624 - acc: 0.9848 - val_loss: 0.5440 - val_acc: 0.8385\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 446us/step - loss: 0.0417 - acc: 0.9908 - val_loss: 0.5984 - val_acc: 0.8300\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 437us/step - loss: 0.0286 - acc: 0.9944 - val_loss: 0.6485 - val_acc: 0.8335\n",
      "20000/20000 [==============================] - 7s 366us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.4100 - acc: 0.8167 - val_loss: 0.3515 - val_acc: 0.8465\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 447us/step - loss: 0.3139 - acc: 0.8665 - val_loss: 0.3485 - val_acc: 0.8495\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 439us/step - loss: 0.2734 - acc: 0.8849 - val_loss: 0.3631 - val_acc: 0.8492\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 443us/step - loss: 0.2320 - acc: 0.9053 - val_loss: 0.3655 - val_acc: 0.8458\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 443us/step - loss: 0.1843 - acc: 0.9306 - val_loss: 0.3925 - val_acc: 0.8380\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 449us/step - loss: 0.1353 - acc: 0.9537 - val_loss: 0.4320 - val_acc: 0.8393\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 447us/step - loss: 0.0937 - acc: 0.9707 - val_loss: 0.4844 - val_acc: 0.8315\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 439us/step - loss: 0.0654 - acc: 0.9815 - val_loss: 0.5473 - val_acc: 0.8300\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 441us/step - loss: 0.0470 - acc: 0.9897 - val_loss: 0.6105 - val_acc: 0.8325\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 440us/step - loss: 0.0333 - acc: 0.9928 - val_loss: 0.6691 - val_acc: 0.8280\n",
      "20000/20000 [==============================] - 7s 362us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.4156 - acc: 0.8113 - val_loss: 0.3501 - val_acc: 0.8492\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 8s 518us/step - loss: 0.3137 - acc: 0.8665 - val_loss: 0.3520 - val_acc: 0.8482\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 8s 482us/step - loss: 0.2776 - acc: 0.8829 - val_loss: 0.3636 - val_acc: 0.8438\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 452us/step - loss: 0.2380 - acc: 0.9040 - val_loss: 0.3661 - val_acc: 0.8512\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 425us/step - loss: 0.1900 - acc: 0.9280 - val_loss: 0.3987 - val_acc: 0.8452\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 8s 475us/step - loss: 0.1454 - acc: 0.9491 - val_loss: 0.4468 - val_acc: 0.8433\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 8s 487us/step - loss: 0.1057 - acc: 0.9665 - val_loss: 0.4904 - val_acc: 0.8400\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 8s 485us/step - loss: 0.0768 - acc: 0.9768 - val_loss: 0.5229 - val_acc: 0.8317\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 448us/step - loss: 0.0549 - acc: 0.9862 - val_loss: 0.5866 - val_acc: 0.8290\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 451us/step - loss: 0.0439 - acc: 0.9890 - val_loss: 0.6156 - val_acc: 0.8310\n",
      "20000/20000 [==============================] - 8s 407us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.4116 - acc: 0.8169 - val_loss: 0.3541 - val_acc: 0.8505\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 401us/step - loss: 0.3145 - acc: 0.8678 - val_loss: 0.3546 - val_acc: 0.8462\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 400us/step - loss: 0.2778 - acc: 0.8822 - val_loss: 0.3599 - val_acc: 0.8468\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 399us/step - loss: 0.2408 - acc: 0.8997 - val_loss: 0.3642 - val_acc: 0.8515\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 420us/step - loss: 0.1931 - acc: 0.9266 - val_loss: 0.4057 - val_acc: 0.8413\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 428us/step - loss: 0.1478 - acc: 0.9469 - val_loss: 0.4362 - val_acc: 0.8390\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 436us/step - loss: 0.1076 - acc: 0.9642 - val_loss: 0.4757 - val_acc: 0.8345\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 447us/step - loss: 0.0822 - acc: 0.9753 - val_loss: 0.5295 - val_acc: 0.8293\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 434us/step - loss: 0.0587 - acc: 0.9832 - val_loss: 0.6111 - val_acc: 0.8283\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 416us/step - loss: 0.0447 - acc: 0.9896 - val_loss: 0.6633 - val_acc: 0.8297\n",
      "20000/20000 [==============================] - 7s 355us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.4134 - acc: 0.8111 - val_loss: 0.3515 - val_acc: 0.8505\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 424us/step - loss: 0.3123 - acc: 0.8667 - val_loss: 0.3510 - val_acc: 0.8518\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 442us/step - loss: 0.2794 - acc: 0.8808 - val_loss: 0.3518 - val_acc: 0.8530\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 431us/step - loss: 0.2402 - acc: 0.9007 - val_loss: 0.3711 - val_acc: 0.8472\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 441us/step - loss: 0.1980 - acc: 0.9234 - val_loss: 0.4019 - val_acc: 0.8470\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 451us/step - loss: 0.1573 - acc: 0.9432 - val_loss: 0.4306 - val_acc: 0.8397\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 446us/step - loss: 0.1188 - acc: 0.9611 - val_loss: 0.4674 - val_acc: 0.8375\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 447us/step - loss: 0.0891 - acc: 0.9732 - val_loss: 0.5175 - val_acc: 0.8330\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 461us/step - loss: 0.0683 - acc: 0.9801 - val_loss: 0.5767 - val_acc: 0.8377\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 459us/step - loss: 0.0535 - acc: 0.9850 - val_loss: 0.6255 - val_acc: 0.8387\n",
      "20000/20000 [==============================] - 8s 382us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.4091 - acc: 0.8152 - val_loss: 0.3496 - val_acc: 0.8515\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 446us/step - loss: 0.3142 - acc: 0.8649 - val_loss: 0.3521 - val_acc: 0.8520\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 446us/step - loss: 0.2821 - acc: 0.8780 - val_loss: 0.3561 - val_acc: 0.8535\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 447us/step - loss: 0.2471 - acc: 0.8984 - val_loss: 0.3676 - val_acc: 0.8485\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 8s 476us/step - loss: 0.2093 - acc: 0.9172 - val_loss: 0.3918 - val_acc: 0.8440\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 449us/step - loss: 0.1688 - acc: 0.9390 - val_loss: 0.4083 - val_acc: 0.8370\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 459us/step - loss: 0.1310 - acc: 0.9547 - val_loss: 0.4544 - val_acc: 0.8373\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 447us/step - loss: 0.1002 - acc: 0.9675 - val_loss: 0.5027 - val_acc: 0.8300\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 447us/step - loss: 0.0750 - acc: 0.9780 - val_loss: 0.5675 - val_acc: 0.8280\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 451us/step - loss: 0.0619 - acc: 0.9818 - val_loss: 0.5893 - val_acc: 0.8210\n",
      "20000/20000 [==============================] - 8s 377us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.4123 - acc: 0.8104 - val_loss: 0.3493 - val_acc: 0.8508\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 450us/step - loss: 0.3182 - acc: 0.8650 - val_loss: 0.3509 - val_acc: 0.8485\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 8s 491us/step - loss: 0.2860 - acc: 0.8784 - val_loss: 0.3579 - val_acc: 0.8485\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 8s 474us/step - loss: 0.2513 - acc: 0.8946 - val_loss: 0.3662 - val_acc: 0.8452\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 8s 486us/step - loss: 0.2147 - acc: 0.9156 - val_loss: 0.3735 - val_acc: 0.8435\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 454us/step - loss: 0.1742 - acc: 0.9327 - val_loss: 0.4214 - val_acc: 0.8403\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 8s 499us/step - loss: 0.1380 - acc: 0.9534 - val_loss: 0.4445 - val_acc: 0.8383\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 8s 473us/step - loss: 0.1064 - acc: 0.9652 - val_loss: 0.4963 - val_acc: 0.8370\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 8s 486us/step - loss: 0.0799 - acc: 0.9750 - val_loss: 0.5367 - val_acc: 0.8313\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 442us/step - loss: 0.0625 - acc: 0.9815 - val_loss: 0.5786 - val_acc: 0.8310\n",
      "20000/20000 [==============================] - 8s 399us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.4164 - acc: 0.8136 - val_loss: 0.3518 - val_acc: 0.8470\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 8s 471us/step - loss: 0.3187 - acc: 0.8647 - val_loss: 0.3444 - val_acc: 0.8532\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 8s 531us/step - loss: 0.2851 - acc: 0.8764 - val_loss: 0.3516 - val_acc: 0.8528\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 9s 533us/step - loss: 0.2550 - acc: 0.8944 - val_loss: 0.3620 - val_acc: 0.8452\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 8s 499us/step - loss: 0.2194 - acc: 0.9111 - val_loss: 0.3788 - val_acc: 0.8485\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 8s 479us/step - loss: 0.1845 - acc: 0.9290 - val_loss: 0.3999 - val_acc: 0.8430\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 463us/step - loss: 0.1451 - acc: 0.9476 - val_loss: 0.4336 - val_acc: 0.8370\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 434us/step - loss: 0.1139 - acc: 0.9631 - val_loss: 0.5047 - val_acc: 0.8375\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 431us/step - loss: 0.0884 - acc: 0.9717 - val_loss: 0.5094 - val_acc: 0.8265\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 438us/step - loss: 0.0720 - acc: 0.9788 - val_loss: 0.5720 - val_acc: 0.8330\n",
      "20000/20000 [==============================] - 8s 381us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.4181 - acc: 0.8098 - val_loss: 0.3503 - val_acc: 0.8478\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 428us/step - loss: 0.3164 - acc: 0.8653 - val_loss: 0.3458 - val_acc: 0.8522\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 445us/step - loss: 0.2826 - acc: 0.8782 - val_loss: 0.3639 - val_acc: 0.8450\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 430us/step - loss: 0.2507 - acc: 0.8950 - val_loss: 0.3745 - val_acc: 0.8500\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 418us/step - loss: 0.2157 - acc: 0.9133 - val_loss: 0.3819 - val_acc: 0.8400\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 413us/step - loss: 0.1726 - acc: 0.9366 - val_loss: 0.4059 - val_acc: 0.8410\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 446us/step - loss: 0.1424 - acc: 0.9481 - val_loss: 0.4471 - val_acc: 0.8340\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 448us/step - loss: 0.1113 - acc: 0.9629 - val_loss: 0.4923 - val_acc: 0.8340\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 434us/step - loss: 0.0871 - acc: 0.9712 - val_loss: 0.5184 - val_acc: 0.8303\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 427us/step - loss: 0.0726 - acc: 0.9771 - val_loss: 0.5745 - val_acc: 0.8307\n",
      "20000/20000 [==============================] - 8s 388us/step\n",
      "6 9\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 23s 1ms/step - loss: 0.4032 - acc: 0.8192 - val_loss: 0.3582 - val_acc: 0.8488\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 430us/step - loss: 0.3164 - acc: 0.8656 - val_loss: 0.3509 - val_acc: 0.8475\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 426us/step - loss: 0.2781 - acc: 0.8836 - val_loss: 0.3560 - val_acc: 0.8502\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 435us/step - loss: 0.2348 - acc: 0.9042 - val_loss: 0.3642 - val_acc: 0.8445\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 453us/step - loss: 0.1835 - acc: 0.9312 - val_loss: 0.3910 - val_acc: 0.8442\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 436us/step - loss: 0.1356 - acc: 0.9566 - val_loss: 0.4225 - val_acc: 0.8323\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 430us/step - loss: 0.0928 - acc: 0.9737 - val_loss: 0.4858 - val_acc: 0.8295\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 426us/step - loss: 0.0586 - acc: 0.9870 - val_loss: 0.5358 - val_acc: 0.8323\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 407us/step - loss: 0.0399 - acc: 0.9928 - val_loss: 0.5882 - val_acc: 0.8263\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 406us/step - loss: 0.0261 - acc: 0.9965 - val_loss: 0.6541 - val_acc: 0.8295\n",
      "20000/20000 [==============================] - 7s 363us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.4105 - acc: 0.8139 - val_loss: 0.3582 - val_acc: 0.8465\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 440us/step - loss: 0.3151 - acc: 0.8656 - val_loss: 0.3517 - val_acc: 0.8482\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 438us/step - loss: 0.2778 - acc: 0.8820 - val_loss: 0.3582 - val_acc: 0.8530\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 438us/step - loss: 0.2370 - acc: 0.9041 - val_loss: 0.3737 - val_acc: 0.8397\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 439us/step - loss: 0.1877 - acc: 0.9281 - val_loss: 0.3916 - val_acc: 0.8413\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 443us/step - loss: 0.1424 - acc: 0.9509 - val_loss: 0.4397 - val_acc: 0.8387\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 457us/step - loss: 0.0986 - acc: 0.9699 - val_loss: 0.4701 - val_acc: 0.8353\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 441us/step - loss: 0.0681 - acc: 0.9815 - val_loss: 0.5325 - val_acc: 0.8363\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 444us/step - loss: 0.0465 - acc: 0.9903 - val_loss: 0.5776 - val_acc: 0.8353\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 444us/step - loss: 0.0320 - acc: 0.9938 - val_loss: 0.6466 - val_acc: 0.8297\n",
      "20000/20000 [==============================] - 7s 357us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.4117 - acc: 0.8129 - val_loss: 0.3509 - val_acc: 0.8490\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 450us/step - loss: 0.3144 - acc: 0.8662 - val_loss: 0.3511 - val_acc: 0.8548\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 439us/step - loss: 0.2772 - acc: 0.8828 - val_loss: 0.3508 - val_acc: 0.8518\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 439us/step - loss: 0.2331 - acc: 0.9062 - val_loss: 0.3680 - val_acc: 0.8455\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 428us/step - loss: 0.1883 - acc: 0.9276 - val_loss: 0.3979 - val_acc: 0.8415\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 444us/step - loss: 0.1448 - acc: 0.9496 - val_loss: 0.4313 - val_acc: 0.8337\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 447us/step - loss: 0.1032 - acc: 0.9702 - val_loss: 0.4726 - val_acc: 0.8317\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 446us/step - loss: 0.0734 - acc: 0.9791 - val_loss: 0.5159 - val_acc: 0.8307\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 446us/step - loss: 0.0531 - acc: 0.9858 - val_loss: 0.5798 - val_acc: 0.8295\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 445us/step - loss: 0.0397 - acc: 0.9916 - val_loss: 0.6291 - val_acc: 0.8330\n",
      "20000/20000 [==============================] - 7s 369us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.4118 - acc: 0.8147 - val_loss: 0.3432 - val_acc: 0.8522\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 445us/step - loss: 0.3126 - acc: 0.8639 - val_loss: 0.3521 - val_acc: 0.8505\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 449us/step - loss: 0.2734 - acc: 0.8825 - val_loss: 0.3592 - val_acc: 0.8512\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 467us/step - loss: 0.2331 - acc: 0.9052 - val_loss: 0.3750 - val_acc: 0.8407\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 8s 493us/step - loss: 0.1886 - acc: 0.9291 - val_loss: 0.3876 - val_acc: 0.8465\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 445us/step - loss: 0.1456 - acc: 0.9509 - val_loss: 0.4219 - val_acc: 0.8427\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 427us/step - loss: 0.1069 - acc: 0.9667 - val_loss: 0.4860 - val_acc: 0.8290\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 427us/step - loss: 0.0786 - acc: 0.9773 - val_loss: 0.5154 - val_acc: 0.8323\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 427us/step - loss: 0.0588 - acc: 0.9845 - val_loss: 0.5710 - val_acc: 0.8353\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 421us/step - loss: 0.0430 - acc: 0.9909 - val_loss: 0.6086 - val_acc: 0.8230\n",
      "20000/20000 [==============================] - 8s 405us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 24s 2ms/step - loss: 0.4151 - acc: 0.8119 - val_loss: 0.3510 - val_acc: 0.8480\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 432us/step - loss: 0.3171 - acc: 0.8623 - val_loss: 0.3582 - val_acc: 0.8505\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 426us/step - loss: 0.2805 - acc: 0.8791 - val_loss: 0.3539 - val_acc: 0.8508\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 463us/step - loss: 0.2414 - acc: 0.9002 - val_loss: 0.3765 - val_acc: 0.8415\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 9s 539us/step - loss: 0.1985 - acc: 0.9218 - val_loss: 0.3977 - val_acc: 0.8430\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 8s 494us/step - loss: 0.1570 - acc: 0.9439 - val_loss: 0.4207 - val_acc: 0.8360\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 9s 549us/step - loss: 0.1177 - acc: 0.9609 - val_loss: 0.4696 - val_acc: 0.8315\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 441us/step - loss: 0.0887 - acc: 0.9728 - val_loss: 0.5126 - val_acc: 0.8317\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 424us/step - loss: 0.0617 - acc: 0.9828 - val_loss: 0.5791 - val_acc: 0.8303\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 432us/step - loss: 0.0497 - acc: 0.9858 - val_loss: 0.6044 - val_acc: 0.8250\n",
      "20000/20000 [==============================] - 7s 359us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.4145 - acc: 0.8107 - val_loss: 0.3505 - val_acc: 0.8480\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 430us/step - loss: 0.3147 - acc: 0.8661 - val_loss: 0.3537 - val_acc: 0.8465\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 438us/step - loss: 0.2790 - acc: 0.8819 - val_loss: 0.3586 - val_acc: 0.8475\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 449us/step - loss: 0.2429 - acc: 0.9000 - val_loss: 0.3666 - val_acc: 0.8455\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 450us/step - loss: 0.2024 - acc: 0.9205 - val_loss: 0.3927 - val_acc: 0.8425\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 450us/step - loss: 0.1562 - acc: 0.9434 - val_loss: 0.4084 - val_acc: 0.8335\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 461us/step - loss: 0.1209 - acc: 0.9582 - val_loss: 0.4655 - val_acc: 0.8347\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 8s 516us/step - loss: 0.0934 - acc: 0.9699 - val_loss: 0.4938 - val_acc: 0.8310\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 442us/step - loss: 0.0675 - acc: 0.9813 - val_loss: 0.5407 - val_acc: 0.8345\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 445us/step - loss: 0.0518 - acc: 0.9871 - val_loss: 0.5805 - val_acc: 0.8247\n",
      "20000/20000 [==============================] - 7s 366us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.4137 - acc: 0.8136 - val_loss: 0.3511 - val_acc: 0.8430\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 443us/step - loss: 0.3172 - acc: 0.8639 - val_loss: 0.3499 - val_acc: 0.8482\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 445us/step - loss: 0.2823 - acc: 0.8809 - val_loss: 0.3528 - val_acc: 0.8558\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 453us/step - loss: 0.2451 - acc: 0.8996 - val_loss: 0.3690 - val_acc: 0.8420\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 457us/step - loss: 0.2102 - acc: 0.9171 - val_loss: 0.3855 - val_acc: 0.8397\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 453us/step - loss: 0.1673 - acc: 0.9374 - val_loss: 0.4206 - val_acc: 0.8400\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 464us/step - loss: 0.1340 - acc: 0.9533 - val_loss: 0.4456 - val_acc: 0.8363\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 463us/step - loss: 0.0987 - acc: 0.9673 - val_loss: 0.4853 - val_acc: 0.8330\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 453us/step - loss: 0.0787 - acc: 0.9765 - val_loss: 0.5226 - val_acc: 0.8325\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 431us/step - loss: 0.0633 - acc: 0.9797 - val_loss: 0.5751 - val_acc: 0.8290\n",
      "20000/20000 [==============================] - 7s 362us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 21s 1ms/step - loss: 0.4147 - acc: 0.8156 - val_loss: 0.3507 - val_acc: 0.8478\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 424us/step - loss: 0.3149 - acc: 0.8632 - val_loss: 0.3519 - val_acc: 0.8530\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 429us/step - loss: 0.2825 - acc: 0.8784 - val_loss: 0.3541 - val_acc: 0.8488\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 451us/step - loss: 0.2462 - acc: 0.8978 - val_loss: 0.3631 - val_acc: 0.8462\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 465us/step - loss: 0.2082 - acc: 0.9184 - val_loss: 0.3887 - val_acc: 0.8485\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 464us/step - loss: 0.1669 - acc: 0.9396 - val_loss: 0.4133 - val_acc: 0.8405\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 449us/step - loss: 0.1307 - acc: 0.9544 - val_loss: 0.4513 - val_acc: 0.8340\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 447us/step - loss: 0.1009 - acc: 0.9671 - val_loss: 0.5033 - val_acc: 0.8363\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 448us/step - loss: 0.0840 - acc: 0.9723 - val_loss: 0.5253 - val_acc: 0.8340\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 448us/step - loss: 0.0622 - acc: 0.9824 - val_loss: 0.6047 - val_acc: 0.8353\n",
      "20000/20000 [==============================] - 7s 373us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.4170 - acc: 0.8099 - val_loss: 0.3482 - val_acc: 0.8470\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 464us/step - loss: 0.3181 - acc: 0.8638 - val_loss: 0.3481 - val_acc: 0.8528\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 460us/step - loss: 0.2839 - acc: 0.8799 - val_loss: 0.3570 - val_acc: 0.8520\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 462us/step - loss: 0.2495 - acc: 0.8971 - val_loss: 0.3559 - val_acc: 0.8535\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 8s 478us/step - loss: 0.2150 - acc: 0.9143 - val_loss: 0.3835 - val_acc: 0.8470\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 451us/step - loss: 0.1770 - acc: 0.9321 - val_loss: 0.4014 - val_acc: 0.8488\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 453us/step - loss: 0.1426 - acc: 0.9501 - val_loss: 0.4546 - val_acc: 0.8410\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 451us/step - loss: 0.1101 - acc: 0.9633 - val_loss: 0.4771 - val_acc: 0.8370\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 452us/step - loss: 0.0917 - acc: 0.9701 - val_loss: 0.5305 - val_acc: 0.8357\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 454us/step - loss: 0.0737 - acc: 0.9776 - val_loss: 0.5701 - val_acc: 0.8370\n",
      "20000/20000 [==============================] - 7s 348us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.4236 - acc: 0.8088 - val_loss: 0.3492 - val_acc: 0.8508\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 368us/step - loss: 0.3195 - acc: 0.8649 - val_loss: 0.3486 - val_acc: 0.8508\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 367us/step - loss: 0.2890 - acc: 0.8762 - val_loss: 0.3559 - val_acc: 0.8515\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 413us/step - loss: 0.2542 - acc: 0.8936 - val_loss: 0.3680 - val_acc: 0.8442\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 409us/step - loss: 0.2227 - acc: 0.9087 - val_loss: 0.3821 - val_acc: 0.8460\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 368us/step - loss: 0.1899 - acc: 0.9262 - val_loss: 0.4058 - val_acc: 0.8472\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 369us/step - loss: 0.1532 - acc: 0.9449 - val_loss: 0.4300 - val_acc: 0.8373\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 370us/step - loss: 0.1228 - acc: 0.9571 - val_loss: 0.4752 - val_acc: 0.8343\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 368us/step - loss: 0.0999 - acc: 0.9658 - val_loss: 0.4987 - val_acc: 0.8285\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 368us/step - loss: 0.0824 - acc: 0.9734 - val_loss: 0.5442 - val_acc: 0.8283\n",
      "20000/20000 [==============================] - 7s 328us/step\n",
      "7 9\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4157 - acc: 0.8109 - val_loss: 0.3536 - val_acc: 0.8465\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 5s 340us/step - loss: 0.3145 - acc: 0.8676 - val_loss: 0.3432 - val_acc: 0.8500\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 5s 338us/step - loss: 0.2748 - acc: 0.8857 - val_loss: 0.3598 - val_acc: 0.8492\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 5s 338us/step - loss: 0.2387 - acc: 0.9021 - val_loss: 0.3699 - val_acc: 0.8495\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 5s 339us/step - loss: 0.1926 - acc: 0.9254 - val_loss: 0.3898 - val_acc: 0.8407\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 5s 337us/step - loss: 0.1444 - acc: 0.9507 - val_loss: 0.4340 - val_acc: 0.8347\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 5s 342us/step - loss: 0.1014 - acc: 0.9695 - val_loss: 0.4895 - val_acc: 0.8333\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 5s 338us/step - loss: 0.0686 - acc: 0.9822 - val_loss: 0.5493 - val_acc: 0.8313\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 5s 338us/step - loss: 0.0469 - acc: 0.9898 - val_loss: 0.6078 - val_acc: 0.8315\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 5s 335us/step - loss: 0.0314 - acc: 0.9946 - val_loss: 0.6519 - val_acc: 0.8305\n",
      "20000/20000 [==============================] - 6s 293us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4124 - acc: 0.8152 - val_loss: 0.3534 - val_acc: 0.8448\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 354us/step - loss: 0.3133 - acc: 0.8673 - val_loss: 0.3529 - val_acc: 0.8450\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 354us/step - loss: 0.2748 - acc: 0.8845 - val_loss: 0.3592 - val_acc: 0.8475\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 398us/step - loss: 0.2335 - acc: 0.9041 - val_loss: 0.3729 - val_acc: 0.8393\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 437us/step - loss: 0.1847 - acc: 0.9300 - val_loss: 0.3868 - val_acc: 0.8407\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 412us/step - loss: 0.1407 - acc: 0.9521 - val_loss: 0.4185 - val_acc: 0.8345\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 350us/step - loss: 0.0990 - acc: 0.9705 - val_loss: 0.4735 - val_acc: 0.8325\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 352us/step - loss: 0.0673 - acc: 0.9839 - val_loss: 0.5044 - val_acc: 0.8325\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 354us/step - loss: 0.0468 - acc: 0.9897 - val_loss: 0.5549 - val_acc: 0.8240\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 351us/step - loss: 0.0332 - acc: 0.9944 - val_loss: 0.6147 - val_acc: 0.8265\n",
      "20000/20000 [==============================] - 6s 284us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4117 - acc: 0.8134 - val_loss: 0.3498 - val_acc: 0.8482\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 346us/step - loss: 0.3142 - acc: 0.8657 - val_loss: 0.3499 - val_acc: 0.8485\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 347us/step - loss: 0.2819 - acc: 0.8821 - val_loss: 0.3598 - val_acc: 0.8460\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 354us/step - loss: 0.2428 - acc: 0.8999 - val_loss: 0.3748 - val_acc: 0.8430\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 378us/step - loss: 0.1990 - acc: 0.9221 - val_loss: 0.3863 - val_acc: 0.8333\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 354us/step - loss: 0.1529 - acc: 0.9461 - val_loss: 0.4217 - val_acc: 0.8403\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 368us/step - loss: 0.1153 - acc: 0.9622 - val_loss: 0.4716 - val_acc: 0.8235\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 450us/step - loss: 0.0829 - acc: 0.9758 - val_loss: 0.5293 - val_acc: 0.8330\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 445us/step - loss: 0.0594 - acc: 0.9841 - val_loss: 0.5805 - val_acc: 0.8273\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 458us/step - loss: 0.0433 - acc: 0.9892 - val_loss: 0.6328 - val_acc: 0.8273\n",
      "20000/20000 [==============================] - 6s 314us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 80s 5ms/step - loss: 0.4071 - acc: 0.8178 - val_loss: 0.3569 - val_acc: 0.8445\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 378us/step - loss: 0.3175 - acc: 0.8618 - val_loss: 0.3499 - val_acc: 0.8498\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 375us/step - loss: 0.2816 - acc: 0.8797 - val_loss: 0.3581 - val_acc: 0.8485\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 380us/step - loss: 0.2446 - acc: 0.8968 - val_loss: 0.3597 - val_acc: 0.8475\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 381us/step - loss: 0.1986 - acc: 0.9217 - val_loss: 0.3902 - val_acc: 0.8387\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 378us/step - loss: 0.1532 - acc: 0.9449 - val_loss: 0.4200 - val_acc: 0.8305\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.1147 - acc: 0.9627 - val_loss: 0.4687 - val_acc: 0.8307\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.0810 - acc: 0.9764 - val_loss: 0.5097 - val_acc: 0.8267\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 377us/step - loss: 0.0619 - acc: 0.9826 - val_loss: 0.5716 - val_acc: 0.8277\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.0455 - acc: 0.9886 - val_loss: 0.6281 - val_acc: 0.8257\n",
      "20000/20000 [==============================] - 7s 346us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4098 - acc: 0.8156 - val_loss: 0.3468 - val_acc: 0.8562\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 361us/step - loss: 0.3156 - acc: 0.8661 - val_loss: 0.3449 - val_acc: 0.8508\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 363us/step - loss: 0.2796 - acc: 0.8808 - val_loss: 0.3581 - val_acc: 0.8510\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 378us/step - loss: 0.2421 - acc: 0.9012 - val_loss: 0.3712 - val_acc: 0.8488\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 408us/step - loss: 0.1985 - acc: 0.9206 - val_loss: 0.3897 - val_acc: 0.8430\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 375us/step - loss: 0.1524 - acc: 0.9464 - val_loss: 0.4162 - val_acc: 0.8375\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 359us/step - loss: 0.1151 - acc: 0.9624 - val_loss: 0.4574 - val_acc: 0.8330\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 363us/step - loss: 0.0865 - acc: 0.9724 - val_loss: 0.5111 - val_acc: 0.8305\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 363us/step - loss: 0.0665 - acc: 0.9805 - val_loss: 0.5541 - val_acc: 0.8355\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 364us/step - loss: 0.0515 - acc: 0.9856 - val_loss: 0.6144 - val_acc: 0.8305\n",
      "20000/20000 [==============================] - 7s 333us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4149 - acc: 0.8152 - val_loss: 0.3485 - val_acc: 0.8510\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 419us/step - loss: 0.3133 - acc: 0.8653 - val_loss: 0.3465 - val_acc: 0.8510\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 397us/step - loss: 0.2800 - acc: 0.8794 - val_loss: 0.3522 - val_acc: 0.8562\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.2432 - acc: 0.9005 - val_loss: 0.3601 - val_acc: 0.8472\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 386us/step - loss: 0.2042 - acc: 0.9188 - val_loss: 0.3855 - val_acc: 0.8433\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.1617 - acc: 0.9406 - val_loss: 0.4071 - val_acc: 0.8340\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 386us/step - loss: 0.1234 - acc: 0.9579 - val_loss: 0.4625 - val_acc: 0.8353\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.0980 - acc: 0.9684 - val_loss: 0.5095 - val_acc: 0.8325\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 383us/step - loss: 0.0722 - acc: 0.9780 - val_loss: 0.5317 - val_acc: 0.8293\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 386us/step - loss: 0.0576 - acc: 0.9830 - val_loss: 0.5950 - val_acc: 0.8313\n",
      "20000/20000 [==============================] - 7s 352us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4172 - acc: 0.8088 - val_loss: 0.3544 - val_acc: 0.8460\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 405us/step - loss: 0.3146 - acc: 0.8658 - val_loss: 0.3501 - val_acc: 0.8485\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 360us/step - loss: 0.2810 - acc: 0.8781 - val_loss: 0.3541 - val_acc: 0.8478\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 345us/step - loss: 0.2467 - acc: 0.8977 - val_loss: 0.3635 - val_acc: 0.8538\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 348us/step - loss: 0.2073 - acc: 0.9173 - val_loss: 0.3813 - val_acc: 0.8415\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 347us/step - loss: 0.1653 - acc: 0.9391 - val_loss: 0.4275 - val_acc: 0.8440\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 349us/step - loss: 0.1332 - acc: 0.9539 - val_loss: 0.4639 - val_acc: 0.8410\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 349us/step - loss: 0.1029 - acc: 0.9666 - val_loss: 0.4897 - val_acc: 0.8347\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 351us/step - loss: 0.0798 - acc: 0.9743 - val_loss: 0.5553 - val_acc: 0.8317\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 345us/step - loss: 0.0647 - acc: 0.9802 - val_loss: 0.5660 - val_acc: 0.8323\n",
      "20000/20000 [==============================] - 6s 295us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4180 - acc: 0.8106 - val_loss: 0.3522 - val_acc: 0.8462\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 360us/step - loss: 0.3201 - acc: 0.8650 - val_loss: 0.3476 - val_acc: 0.8532\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 363us/step - loss: 0.2849 - acc: 0.8802 - val_loss: 0.3564 - val_acc: 0.8495\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 354us/step - loss: 0.2530 - acc: 0.8939 - val_loss: 0.3590 - val_acc: 0.8470\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 355us/step - loss: 0.2160 - acc: 0.9135 - val_loss: 0.3816 - val_acc: 0.8465\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 356us/step - loss: 0.1801 - acc: 0.9323 - val_loss: 0.4027 - val_acc: 0.8413\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 360us/step - loss: 0.1421 - acc: 0.9478 - val_loss: 0.4415 - val_acc: 0.8413\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 357us/step - loss: 0.1128 - acc: 0.9604 - val_loss: 0.5092 - val_acc: 0.8335\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 378us/step - loss: 0.0911 - acc: 0.9711 - val_loss: 0.5539 - val_acc: 0.8305\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 465us/step - loss: 0.0726 - acc: 0.9772 - val_loss: 0.5995 - val_acc: 0.8287\n",
      "20000/20000 [==============================] - 7s 343us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4162 - acc: 0.8119 - val_loss: 0.3507 - val_acc: 0.8485\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.3201 - acc: 0.8650 - val_loss: 0.3504 - val_acc: 0.8518\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 366us/step - loss: 0.2864 - acc: 0.8757 - val_loss: 0.3562 - val_acc: 0.8538\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 366us/step - loss: 0.2558 - acc: 0.8903 - val_loss: 0.3626 - val_acc: 0.8478\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 363us/step - loss: 0.2224 - acc: 0.9098 - val_loss: 0.3785 - val_acc: 0.8440\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 365us/step - loss: 0.1877 - acc: 0.9258 - val_loss: 0.4198 - val_acc: 0.8400\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 367us/step - loss: 0.1549 - acc: 0.9438 - val_loss: 0.4395 - val_acc: 0.8373\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 368us/step - loss: 0.1248 - acc: 0.9572 - val_loss: 0.4707 - val_acc: 0.8405\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 370us/step - loss: 0.1015 - acc: 0.9665 - val_loss: 0.5046 - val_acc: 0.8325\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 396us/step - loss: 0.0840 - acc: 0.9713 - val_loss: 0.5394 - val_acc: 0.8290\n",
      "20000/20000 [==============================] - 7s 327us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4118 - acc: 0.8115 - val_loss: 0.3524 - val_acc: 0.8498\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 361us/step - loss: 0.3186 - acc: 0.8617 - val_loss: 0.3563 - val_acc: 0.8480\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 361us/step - loss: 0.2882 - acc: 0.8792 - val_loss: 0.3588 - val_acc: 0.8482\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 360us/step - loss: 0.2555 - acc: 0.8914 - val_loss: 0.3619 - val_acc: 0.8460\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 392us/step - loss: 0.2231 - acc: 0.9096 - val_loss: 0.3865 - val_acc: 0.8450\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 440us/step - loss: 0.1876 - acc: 0.9284 - val_loss: 0.4078 - val_acc: 0.8357\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.1526 - acc: 0.9421 - val_loss: 0.4486 - val_acc: 0.8393\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 380us/step - loss: 0.1262 - acc: 0.9539 - val_loss: 0.4754 - val_acc: 0.8320\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.0991 - acc: 0.9670 - val_loss: 0.5249 - val_acc: 0.8317\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 380us/step - loss: 0.0844 - acc: 0.9721 - val_loss: 0.5517 - val_acc: 0.8290\n",
      "20000/20000 [==============================] - 7s 331us/step\n",
      "8 9\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4122 - acc: 0.8129 - val_loss: 0.3489 - val_acc: 0.8502\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 361us/step - loss: 0.3156 - acc: 0.8649 - val_loss: 0.3466 - val_acc: 0.8535\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 363us/step - loss: 0.2726 - acc: 0.8826 - val_loss: 0.3530 - val_acc: 0.8478\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 365us/step - loss: 0.2316 - acc: 0.9038 - val_loss: 0.3577 - val_acc: 0.8505\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 363us/step - loss: 0.1872 - acc: 0.9278 - val_loss: 0.3800 - val_acc: 0.8465\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 370us/step - loss: 0.1408 - acc: 0.9503 - val_loss: 0.4127 - val_acc: 0.8403\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 401us/step - loss: 0.0995 - acc: 0.9702 - val_loss: 0.4581 - val_acc: 0.8397\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 363us/step - loss: 0.0704 - acc: 0.9820 - val_loss: 0.5178 - val_acc: 0.8387\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 366us/step - loss: 0.0483 - acc: 0.9889 - val_loss: 0.5824 - val_acc: 0.8375\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 364us/step - loss: 0.0343 - acc: 0.9944 - val_loss: 0.6064 - val_acc: 0.8345\n",
      "20000/20000 [==============================] - 7s 341us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4118 - acc: 0.8122 - val_loss: 0.3505 - val_acc: 0.8488\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 381us/step - loss: 0.3165 - acc: 0.8638 - val_loss: 0.3565 - val_acc: 0.8460\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 420us/step - loss: 0.2787 - acc: 0.8814 - val_loss: 0.3572 - val_acc: 0.8500\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 381us/step - loss: 0.2397 - acc: 0.9000 - val_loss: 0.3684 - val_acc: 0.8500\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 384us/step - loss: 0.1979 - acc: 0.9239 - val_loss: 0.3920 - val_acc: 0.8377\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.1512 - acc: 0.9463 - val_loss: 0.4277 - val_acc: 0.8445\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 382us/step - loss: 0.1091 - acc: 0.9649 - val_loss: 0.4604 - val_acc: 0.8340\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 384us/step - loss: 0.0785 - acc: 0.9768 - val_loss: 0.5190 - val_acc: 0.8370\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.0563 - acc: 0.9868 - val_loss: 0.5615 - val_acc: 0.8370\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 382us/step - loss: 0.0406 - acc: 0.9912 - val_loss: 0.6269 - val_acc: 0.8310\n",
      "20000/20000 [==============================] - 8s 381us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4139 - acc: 0.8136 - val_loss: 0.3558 - val_acc: 0.8472\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 371us/step - loss: 0.3164 - acc: 0.8659 - val_loss: 0.3479 - val_acc: 0.8488\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 376us/step - loss: 0.2762 - acc: 0.8823 - val_loss: 0.3619 - val_acc: 0.8470\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 377us/step - loss: 0.2379 - acc: 0.9015 - val_loss: 0.3689 - val_acc: 0.8462\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 375us/step - loss: 0.1982 - acc: 0.9215 - val_loss: 0.3886 - val_acc: 0.8407\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 392us/step - loss: 0.1538 - acc: 0.9443 - val_loss: 0.4219 - val_acc: 0.8448\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 394us/step - loss: 0.1133 - acc: 0.9636 - val_loss: 0.4707 - val_acc: 0.8337\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 380us/step - loss: 0.0857 - acc: 0.9748 - val_loss: 0.5110 - val_acc: 0.8280\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 379us/step - loss: 0.0631 - acc: 0.9827 - val_loss: 0.5820 - val_acc: 0.8327\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 435us/step - loss: 0.0453 - acc: 0.9887 - val_loss: 0.6222 - val_acc: 0.8293\n",
      "20000/20000 [==============================] - 9s 442us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4137 - acc: 0.8111 - val_loss: 0.3521 - val_acc: 0.8488\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 365us/step - loss: 0.3163 - acc: 0.8639 - val_loss: 0.3501 - val_acc: 0.8482\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 366us/step - loss: 0.2818 - acc: 0.8806 - val_loss: 0.3514 - val_acc: 0.8528\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 362us/step - loss: 0.2451 - acc: 0.8987 - val_loss: 0.3682 - val_acc: 0.8462\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 366us/step - loss: 0.2037 - acc: 0.9207 - val_loss: 0.3930 - val_acc: 0.8452\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 371us/step - loss: 0.1598 - acc: 0.9394 - val_loss: 0.4168 - val_acc: 0.8407\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 368us/step - loss: 0.1195 - acc: 0.9602 - val_loss: 0.4617 - val_acc: 0.8350\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 364us/step - loss: 0.0893 - acc: 0.9725 - val_loss: 0.5149 - val_acc: 0.8347\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 362us/step - loss: 0.0654 - acc: 0.9815 - val_loss: 0.5857 - val_acc: 0.8295\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 364us/step - loss: 0.0506 - acc: 0.9866 - val_loss: 0.6182 - val_acc: 0.8285\n",
      "20000/20000 [==============================] - 7s 331us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4150 - acc: 0.8124 - val_loss: 0.3543 - val_acc: 0.8442\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 366us/step - loss: 0.3193 - acc: 0.8641 - val_loss: 0.3477 - val_acc: 0.8528\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 358us/step - loss: 0.2830 - acc: 0.8774 - val_loss: 0.3548 - val_acc: 0.8490\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 357us/step - loss: 0.2500 - acc: 0.8961 - val_loss: 0.3608 - val_acc: 0.8490\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 356us/step - loss: 0.2111 - acc: 0.9149 - val_loss: 0.3718 - val_acc: 0.8470\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 357us/step - loss: 0.1712 - acc: 0.9360 - val_loss: 0.3966 - val_acc: 0.8425\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 360us/step - loss: 0.1334 - acc: 0.9532 - val_loss: 0.4489 - val_acc: 0.8417\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 434us/step - loss: 0.1018 - acc: 0.9662 - val_loss: 0.4929 - val_acc: 0.8365\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 372us/step - loss: 0.0759 - acc: 0.9778 - val_loss: 0.5624 - val_acc: 0.8255\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 376us/step - loss: 0.0606 - acc: 0.9822 - val_loss: 0.6010 - val_acc: 0.8225\n",
      "20000/20000 [==============================] - 7s 338us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4159 - acc: 0.8143 - val_loss: 0.3487 - val_acc: 0.8512\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 395us/step - loss: 0.3174 - acc: 0.8651 - val_loss: 0.3453 - val_acc: 0.8532\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.2864 - acc: 0.8770 - val_loss: 0.3525 - val_acc: 0.8540\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 393us/step - loss: 0.2556 - acc: 0.8916 - val_loss: 0.3622 - val_acc: 0.8540\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 394us/step - loss: 0.2173 - acc: 0.9133 - val_loss: 0.3820 - val_acc: 0.8440\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 8s 509us/step - loss: 0.1790 - acc: 0.9304 - val_loss: 0.4176 - val_acc: 0.8403\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 451us/step - loss: 0.1417 - acc: 0.9497 - val_loss: 0.4558 - val_acc: 0.8355\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 418us/step - loss: 0.1079 - acc: 0.9629 - val_loss: 0.4774 - val_acc: 0.8293\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 400us/step - loss: 0.0860 - acc: 0.9720 - val_loss: 0.5430 - val_acc: 0.8335\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 396us/step - loss: 0.0681 - acc: 0.9783 - val_loss: 0.5901 - val_acc: 0.8273\n",
      "20000/20000 [==============================] - 7s 339us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4116 - acc: 0.8174 - val_loss: 0.3586 - val_acc: 0.8440\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 408us/step - loss: 0.3187 - acc: 0.8626 - val_loss: 0.3550 - val_acc: 0.8433\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 367us/step - loss: 0.2867 - acc: 0.8759 - val_loss: 0.3503 - val_acc: 0.8490\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 6s 366us/step - loss: 0.2538 - acc: 0.8958 - val_loss: 0.3650 - val_acc: 0.8482\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 367us/step - loss: 0.2140 - acc: 0.9154 - val_loss: 0.3829 - val_acc: 0.8480\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 370us/step - loss: 0.1725 - acc: 0.9334 - val_loss: 0.4084 - val_acc: 0.8427\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 366us/step - loss: 0.1385 - acc: 0.9489 - val_loss: 0.4549 - val_acc: 0.8363\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 365us/step - loss: 0.1090 - acc: 0.9626 - val_loss: 0.4767 - val_acc: 0.8255\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 366us/step - loss: 0.0872 - acc: 0.9710 - val_loss: 0.5356 - val_acc: 0.8285\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 369us/step - loss: 0.0667 - acc: 0.9791 - val_loss: 0.5935 - val_acc: 0.8317\n",
      "20000/20000 [==============================] - 7s 342us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4140 - acc: 0.8106 - val_loss: 0.3462 - val_acc: 0.8582\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 6s 362us/step - loss: 0.3158 - acc: 0.8654 - val_loss: 0.3538 - val_acc: 0.8510\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 6s 388us/step - loss: 0.2824 - acc: 0.8799 - val_loss: 0.3547 - val_acc: 0.8532\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 457us/step - loss: 0.2501 - acc: 0.8971 - val_loss: 0.3680 - val_acc: 0.8462\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 6s 385us/step - loss: 0.2155 - acc: 0.9134 - val_loss: 0.3824 - val_acc: 0.8370\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 390us/step - loss: 0.1792 - acc: 0.9310 - val_loss: 0.4122 - val_acc: 0.8442\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 389us/step - loss: 0.1438 - acc: 0.9498 - val_loss: 0.4314 - val_acc: 0.8340\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 389us/step - loss: 0.1090 - acc: 0.9617 - val_loss: 0.4643 - val_acc: 0.8377\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 389us/step - loss: 0.0863 - acc: 0.9716 - val_loss: 0.5234 - val_acc: 0.8303\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 389us/step - loss: 0.0722 - acc: 0.9766 - val_loss: 0.5579 - val_acc: 0.8370\n",
      "20000/20000 [==============================] - 7s 359us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 22s 1ms/step - loss: 0.4153 - acc: 0.8101 - val_loss: 0.3483 - val_acc: 0.8498\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 7s 449us/step - loss: 0.3188 - acc: 0.8642 - val_loss: 0.3498 - val_acc: 0.8522\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 457us/step - loss: 0.2891 - acc: 0.8781 - val_loss: 0.3558 - val_acc: 0.8515\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 8s 486us/step - loss: 0.2587 - acc: 0.8916 - val_loss: 0.3675 - val_acc: 0.8492\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 451us/step - loss: 0.2265 - acc: 0.9069 - val_loss: 0.3782 - val_acc: 0.8438\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 6s 364us/step - loss: 0.1876 - acc: 0.9253 - val_loss: 0.4057 - val_acc: 0.8425\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 6s 378us/step - loss: 0.1571 - acc: 0.9439 - val_loss: 0.4287 - val_acc: 0.8427\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 6s 367us/step - loss: 0.1278 - acc: 0.9526 - val_loss: 0.4795 - val_acc: 0.8353\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 6s 362us/step - loss: 0.1043 - acc: 0.9646 - val_loss: 0.5005 - val_acc: 0.8310\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 6s 364us/step - loss: 0.0846 - acc: 0.9715 - val_loss: 0.5626 - val_acc: 0.8325\n",
      "20000/20000 [==============================] - 7s 332us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4199 - acc: 0.8076 - val_loss: 0.3462 - val_acc: 0.8530\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 8s 481us/step - loss: 0.3195 - acc: 0.8626 - val_loss: 0.3440 - val_acc: 0.8518\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 7s 452us/step - loss: 0.2886 - acc: 0.8756 - val_loss: 0.3523 - val_acc: 0.8522\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 7s 424us/step - loss: 0.2631 - acc: 0.8901 - val_loss: 0.3660 - val_acc: 0.8458\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 7s 412us/step - loss: 0.2317 - acc: 0.9054 - val_loss: 0.3718 - val_acc: 0.8498\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 7s 423us/step - loss: 0.1969 - acc: 0.9239 - val_loss: 0.3970 - val_acc: 0.8438\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 7s 421us/step - loss: 0.1637 - acc: 0.9374 - val_loss: 0.4204 - val_acc: 0.8448\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 7s 420us/step - loss: 0.1369 - acc: 0.9507 - val_loss: 0.4636 - val_acc: 0.8395\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 7s 437us/step - loss: 0.1120 - acc: 0.9608 - val_loss: 0.5014 - val_acc: 0.8395\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 7s 428us/step - loss: 0.0921 - acc: 0.9684 - val_loss: 0.5280 - val_acc: 0.8380\n",
      "20000/20000 [==============================] - 7s 372us/step\n",
      "9 9\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "accuracy = []\n",
    "totalscores = []\n",
    "accuracies = []\n",
    "for j in range(10):\n",
    "    for i in range(10):\n",
    "        # Build sequential model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Hidden layers\n",
    "        model.add(Dense(500, activation=\"tanh\", input_shape=(1000,)))\n",
    "        model.add(Dropout(0.025 * j))\n",
    "        model.add(Dense(100, activation=\"relu\"))\n",
    "        model.add(Dropout(.025*i))\n",
    "\n",
    "        # Output layer\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        \n",
    "        # Compile\n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "        fit = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2, shuffle=True, verbose=1)\n",
    "        score = model.evaluate(X_train, y_train, verbose=1)\n",
    "        scores.append(score[0])\n",
    "        accuracy.append(score[1])\n",
    "    totalscores.append(scores)\n",
    "    accuracies.append(accuracy)\n",
    "    scores = []\n",
    "    accuracy = []\n",
    "    print( j,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(totalscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4oJgZmaAC4KZmSUuCGZmBsD/AeljneTPTdklAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f59e5dfef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anewrange = np.multiply(range(10),.025)\n",
    "for i in range(10):\n",
    "    plt.plot(anewrange,accuracies[i],label = anewrange[i])\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4097 - acc: 0.8137 - val_loss: 0.3553 - val_acc: 0.8498\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 6s 396us/step - loss: 0.3129 - acc: 0.8654 - val_loss: 0.3469 - val_acc: 0.8545\n",
      "20000/20000 [==============================] - 7s 355us/step\n",
      "Test score: 0.27085585519075395\n",
      "Test accuracy: 0.88955\n"
     ]
    }
   ],
   "source": [
    "# Build sequential model\n",
    "model = Sequential()\n",
    "\n",
    "        # Hidden layers\n",
    "model.add(Dense(500, activation=\"tanh\", input_shape=(1000,)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dropout(.1))\n",
    "\n",
    " # Output layer\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "# Compile\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "fit = model.fit(X_train, y_train, batch_size=128, epochs=2, validation_split=0.2, shuffle=True, verbose=1)\n",
    "score = model.evaluate(X_train, y_train, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 6s 320us/step\n",
      "Test score: 0.25915315672159195\n",
      "Test accuracy: 0.89765\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_train, y_train, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.loadtxt('test_data.txt',skiprows = 1)\n",
    "pred = model.predict(X_test)\n",
    "super_thresh = pred >= .5\n",
    "pred.fill(0)\n",
    "pred[super_thresh] = 1\n",
    "submission = [[i+1, int(pred[i])] for i in range(len(pred))]\n",
    "submission.insert(0, ['Id','Prediction'])\n",
    "with open('submission.csv', 'w') as f:\n",
    "    for line in submission:\n",
    "        f.write(','.join(map(str, line)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.952728  ],\n",
       "       [0.8396885 ],\n",
       "       [0.30971947],\n",
       "       ...,\n",
       "       [0.00741125],\n",
       "       [0.9782658 ],\n",
       "       [0.09041308]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
